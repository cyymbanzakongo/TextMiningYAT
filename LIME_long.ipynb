{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from pathlib import Path\n",
    "import nltk, spacy\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import STOPWORDS\n",
    "from collections import Counter\n",
    "from gensim.parsing.preprocessing import STOPWORDS as SW\n",
    "\n",
    "pd.options.display.float_format = '{:,.0f}'.format\n",
    "in_file = Path.cwd().parents[0] / 'Processed_datasets' / 'final_db.csv'\n",
    "my_stopwords = stopwords.words('english')\n",
    "stopwords = set(STOPWORDS).union(my_stopwords) #preparing stopwards list\n",
    "custom_stopwords = ['hi', '\\n', '\\n\\n', '&amp;', ' ', '.', '-',\n",
    "                    'got', \"it's\", 'it‚Äôs', \"i'm\", 'i‚Äôm', 'im', 'want', 'like', '$', '@']\n",
    "nlp = spacy.load('en_core_web_sm', exclude=['ner', 'tok2vec', 'tagger', 'paerser', 'senter', 'lemmatizer', 'attribute_ruler']) # using only for stopwords\n",
    "STOP_WORDS = nlp.Defaults.stop_words.union(custom_stopwords)\n",
    "ALL_STOP_WORDS = STOP_WORDS.union(SW).union(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amaan\\coding\\Text_minig\\Project\\TextMiningYAT\\Processed_datasets\\final_db.csv\n"
     ]
    }
   ],
   "source": [
    "#1. build a random forest classifier with 'longer' text\n",
    "import pandas as pd\n",
    "in_file = Path.cwd() / 'Processed_datasets' / 'final_db.csv'\n",
    "print(in_file)\n",
    "df_train = pd.read_csv(in_file, engine='python', usecols=['Text', 'oh_label'], encoding='utf-8') #not using unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"White Sox are in first place. Some of you Sox fans are so spoiled. You talk like Jim Hendry came over from the Cubs and you havent won in a hundred years. Get a grip.\" Ready to run playing as mkr background music üòçüëå #mkr\n",
      "<class 'str'>\n",
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#sub_df_train = df_train[['Tokens','Label']].copy()\n",
    "#label_map = dict(zip(df_train['Label'].unique().tolist(), range(6)))\n",
    "#sub_df_train['Flipped_label'] = sub_df_train['Label'].apply(lambda x: label_map[x])\n",
    "xs = df_train['Text'].values\n",
    "ys = df_train['oh_label'].values\n",
    "train_x, test_x, train_y, test_y = train_test_split(xs, ys, random_state=42, test_size=0.15, stratify=ys)\n",
    "print(train_x[2005], test_x[2005])\n",
    "print(type(train_x[2005]))\n",
    "print(type(train_y[2005]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to run playing as mkr background music üòçüëå #mkr 0.0\n"
     ]
    }
   ],
   "source": [
    "print(test_x[2005], test_y[2005]\n",
    "     \n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.pipeline import make_pipeline\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "\n",
    "PARAM_GRID = [{\n",
    "    'tfidfvectorizer__stop_words' : [None, ALL_STOP_WORDS]\n",
    "}]\n",
    "\n",
    "tknzr = TweetTokenizer(preserve_case=True, reduce_len=True, strip_handles=True)\n",
    "vect = TfidfVectorizer(max_df=.9, min_df=25, strip_accents='unicode', tokenizer=tknzr.tokenize)\n",
    "sampler = RandomUnderSampler(random_state=42) \n",
    "clf = RandomForestClassifier()\n",
    "c = make_pipeline(vect, sampler, clf)\n",
    "grid_srch = GridSearchCV(\n",
    "    estimator=c, param_grid=PARAM_GRID, refit=True, n_jobs=-1)\n",
    "fitted = grid_srch.fit(train_x, train_y)\n",
    "best_est = fitted.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#vect_2 = CountVectorizer(stop_words='english', max_features=100)\n",
    "#c2 = make_pipeline(vect_2, clf)\n",
    "#c2.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.53       0.47      ]\n",
      " [0.45       0.55      ]\n",
      " [0.74       0.26      ]\n",
      " [0.89292469 0.10707531]\n",
      " [0.81       0.19      ]\n",
      " [0.86       0.14      ]\n",
      " [0.70120172 0.29879828]\n",
      " [0.61       0.39      ]\n",
      " [0.72       0.28      ]\n",
      " [0.45       0.55      ]]\n"
     ]
    }
   ],
   "source": [
    "print (best_est.predict_proba(test_x[0:10]))#, c2.predict_proba(test_x[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier()\n"
     ]
    }
   ],
   "source": [
    "best_clf = best_est[-1]\n",
    "print(best_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tfidfvectorizer',\n",
      "                 TfidfVectorizer(max_df=0.9, min_df=25, strip_accents='unicode',\n",
      "                                 tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x000001FFAA3BAE08>>)),\n",
      "                ('randomundersampler', RandomUnderSampler(random_state=42)),\n",
      "                ('randomforestclassifier', RandomForestClassifier())])\n"
     ]
    }
   ],
   "source": [
    "print(best_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "explainer = LimeTextExplainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document id: 21\n",
      "Probability(non-toxic) = 0.94\n",
      "Tweet: @brendlewhat @MaxBlumenthal @veganforareason @dhere Christians and Hindus murdered for blasphemy by Muslim mobs. http://t.co/f3aWjksaUz\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "idx = 21\n",
    "exp = explainer.explain_instance(test_x[idx], best_est.predict_proba, num_features=6, )\n",
    "print('Document id: %d' % idx)\n",
    "print('Probability(non-toxic) =', best_est.predict_proba([test_x[idx]])[0, 1])\n",
    "#print('True class: %s' % class_names[int(test_y[idx])])\n",
    "print('Tweet: %s' % test_x[idx])\n",
    "print(type(test_x[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Muslim', 0.39325245053412766),\n",
       " ('murdered', 0.14137890877815099),\n",
       " ('Christians', 0.06392280222932924),\n",
       " ('and', 0.05182008638622923),\n",
       " ('Hindus', 0.04530388881058074),\n",
       " ('dhere', 0.03132983621157049)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Haley, all of your interviews are great, because you are great!\\nand wonderful! and beautiful! and ... I love you.\"\n"
     ]
    }
   ],
   "source": [
    "print(test_x[500\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hate', 0.15131205521371688), ('shadowrun', -0.05346704258417674)] [('shadowrun', 1.1859480308508826e-30), ('hate', 1.1745284392340069e-30)]\n"
     ]
    }
   ],
   "source": [
    "exp_2 = explainer.explain_instance(test_x[idx], c.predict_proba, num_features=6)\n",
    "print(exp.as_list(), exp_2.as_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
