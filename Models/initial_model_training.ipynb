{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import nltk, spacy\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import STOPWORDS\n",
    "from collections import Counter\n",
    "from gensim.parsing.preprocessing import STOPWORDS as SW\n",
    "\n",
    "pd.options.display.float_format = '{:,.0f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "in_file = Path.cwd().parents[0] / 'Processed_datasets' / 'tweets_long_no_duplicates.csv'\n",
    "my_stopwords = stopwords.words('english')\n",
    "stopwords = set(STOPWORDS).union(my_stopwords) #preparing stopwards list\n",
    "custom_stopwords = ['hi', '\\n', '\\n\\n', '&amp;', ' ', '.', '-',\n",
    "                    'got', \"it's\", 'it’s', \"i'm\", 'i’m', 'im', 'want', 'like', '$', '@']\n",
    "nlp = spacy.load('en_core_web_sm', exclude=['ner', 'tok2vec', 'tagger', 'paerser', 'senter', 'lemmatizer', 'attribute_ruler']) # using only for stopwords\n",
    "STOP_WORDS = nlp.Defaults.stop_words.union(custom_stopwords)\n",
    "ALL_STOP_WORDS = STOP_WORDS.union(SW).union(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = pd.read_csv(in_file, engine='python', usecols=['Tokens', 'Label']) #not using unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Tokens  Label\n",
      "0   muslim mob violence against hindus in banglade...      1\n",
      "1   islamophobia is like the idea of naziphobia is...      1\n",
      "2   finally all caught up and that sudden death co...      0\n",
      "3   please please start using is your discernment ...      0\n",
      "4   as soon as isis chased all the minorities out ...      0\n",
      "5   islam invaded and conquered of christiandom be...      1\n",
      "6   do you approve of your pedophile prophet rapin...      1\n",
      "7   problem with vile muslims is that they try to ...      1\n",
      "8            tend to talk about it much personal info      0\n",
      "9   cool next time when a woman talks to him about...      0\n",
      "10  our judges are about to turn the heat up in th...      0\n",
      "11  lol this you walk by putting one foot in front...      0\n",
      "12  said wanted sorbet now and they should tell us...      0\n",
      "13  this fucking potato is blowing my mind duck fa...      0\n",
      "14  omg this churner feels like razor blades on my...      0\n",
      "0    15214\n",
      "1     7500\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_long.head(15))\n",
    "df_long.count()\n",
    "print(df_long['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#setting up baseline pipeline\n",
    "from nltk.tokenize import TweetTokenizer #I chose to tokenize with this, as it gets rid of @ handlers\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_dataset(d_frame, stopwords=True, tfidf=True, train=True): #function for creating dataset, paramaters for flexibility\n",
    "    \"\"\"Helper function for construction the classes later on - accepts a dataframe \n",
    "    as an input and 3 booleans and ouputs a tuple of either the test or dev set\n",
    "    The code block essentially repeates itself with minor argument tweaks\n",
    "    When stopwords=True then stopwords are removed, when tfidf=True then tfidf     is implemented and when train-True then train set is returned\"\"\"\n",
    "    df = d_frame\n",
    "    if stopwords and tfidf and train:                                                       #all included\n",
    "        tknzr = TweetTokenizer(preserve_case=True, reduce_len=True)\n",
    "        vect = TfidfVectorizer(max_df=.9, min_df=25, strip_accents='unicode',\n",
    "                               tokenizer=tknzr.tokenize, stop_words=ALL_STOP_WORDS)\n",
    "        sampler_train = RandomUnderSampler(random_state=42) #undersampling  object to rebalance dataset\n",
    "        xs, ys = vect.fit_transform(df['Tokens'].values), df['Label'].values\n",
    "        x_train, x_dev, y_train, y_dev = train_test_split(xs, ys, train_size=.85, #creating train and dev sets with stratified sampling\n",
    "                                                          random_state=42, stratify=ys) \n",
    "        x_train, y_train = sampler_train.fit_resample(x_train, y_train) \n",
    "        return x_train, y_train # x_train is sparse matrix, y_train is array\n",
    "    elif not stopwords and tfidf and train: #no stopwords\n",
    "        tknzr = TweetTokenizer(preserve_case=True, reduce_len=True)\n",
    "        vect = TfidfVectorizer(max_df=.9, min_df=25,strip_accents='unicode',\n",
    "                               tokenizer=tknzr.tokenize)\n",
    "        sampler_train = RandomUnderSampler(random_state=42) \n",
    "        xs, ys = vect.fit_transform(df['Tokens'].values), df['Label'].values\n",
    "        x_train, x_dev, y_train, y_dev = train_test_split(xs, ys, train_size=.85,\n",
    "                                                          random_state=42, stratify=ys)\n",
    "        x_train, y_train = sampler_train.fit_resample(x_train, y_train) \n",
    "        return x_train, y_train    \n",
    "    elif stopwords and not tfidf and train: #no tfidf\n",
    "        tknzr = TweetTokenizer(preserve_case=True, reduce_len=True)\n",
    "        vect = CountVectorizer(max_df=.9, min_df=25,\n",
    "                       strip_accents='unicode', tokenizer=tknzr.tokenize, stop_words=ALL_STOP_WORDS)\n",
    "        sampler_train = RandomUnderSampler(random_state=42) \n",
    "        xs, ys = vect.fit_transform(df['Tokens'].values), df['Label'].values\n",
    "        x_train, x_dev, y_train, y_dev = train_test_split(xs, ys, train_size=.85,\n",
    "                                                          random_state=42, stratify=ys)\n",
    "        x_train, y_train = sampler_train.fit_resample(x_train, y_train) \n",
    "        return x_train, y_train\n",
    "    elif not stopwords and not tfidf and train:\n",
    "        tknzr = TweetTokenizer(preserve_case=True, reduce_len=True)\n",
    "        vect = CountVectorizer(max_df=.9, min_df=25,\n",
    "                       strip_accents='unicode', tokenizer=tknzr.tokenize)\n",
    "        sampler_train = RandomUnderSampler(random_state=42) \n",
    "        xs, ys = vect.fit_transform(df['Tokens'].values), df['Label'].values\n",
    "        x_train, x_dev, y_train, y_dev = train_test_split(xs, ys, train_size=.85,\n",
    "                                                          random_state=42, stratify=ys)\n",
    "        x_train, y_train = sampler_train.fit_resample(x_train, y_train) \n",
    "        return x_train, y_train\n",
    "    elif stopwords and tfidf and not train:\n",
    "        tknzr = TweetTokenizer(preserve_case=True, reduce_len=True)\n",
    "        vect = TfidfVectorizer(max_df=.9, min_df=25,\n",
    "                       strip_accents='unicode', tokenizer=tknzr.tokenize, stop_words=ALL_STOP_WORDS)\n",
    "        sampler_dev = RandomUnderSampler(random_state=50) \n",
    "        xs, ys = vect.fit_transform(df['Tokens'].values), df['Label'].values\n",
    "        x_train, x_dev, y_train, y_dev = train_test_split(xs, ys, train_size=.85,\n",
    "                                                          random_state=42, stratify=ys)\n",
    "        x_dev, y_dev = sampler_dev.fit_resample(x_dev, y_dev)\n",
    "        return x_dev, y_dev\n",
    "    elif not stopwords and tfidf and not train:\n",
    "        tknzr = TweetTokenizer(preserve_case=True, reduce_len=True)\n",
    "        vect = TfidfVectorizer(max_df=.9, min_df=25,strip_accents='unicode',\n",
    "                               tokenizer=tknzr.tokenize)\n",
    "        sampler_dev = RandomUnderSampler(random_state=50) \n",
    "        xs, ys = vect.fit_transform(df['Tokens'].values), df['Label'].values\n",
    "        x_train, x_dev, y_train, y_dev = train_test_split(xs, ys, train_size=.85,\n",
    "                                                          random_state=42, stratify=ys)\n",
    "        x_dev, y_dev = sampler_dev.fit_resample(x_dev, y_dev)\n",
    "        return x_dev, y_dev\n",
    "    elif stopwords and not tfidf and not train:\n",
    "        tknzr = TweetTokenizer(preserve_case=True, reduce_len=True)\n",
    "        vect = CountVectorizer(max_df=.9, min_df=25,strip_accents='unicode',\n",
    "                               tokenizer=tknzr.tokenize, stop_words=ALL_STOP_WORDS)\n",
    "        sampler_dev = RandomUnderSampler(random_state=50) \n",
    "        xs, ys = vect.fit_transform(df['Tokens'].values), df['Label'].values\n",
    "        x_train, x_dev, y_train, y_dev = train_test_split(xs, ys, train_size=.85,\n",
    "                                                          random_state=42, stratify=ys)\n",
    "        x_dev, y_dev = sampler_dev.fit_resample(x_dev, y_dev)\n",
    "        return x_dev, y_dev\n",
    "    else:\n",
    "        tknzr = TweetTokenizer(preserve_case=True, reduce_len=True)\n",
    "        vect = CountVectorizer(max_df=.9, min_df=25,strip_accents='unicode',\n",
    "                               tokenizer=tknzr.tokenize)\n",
    "        sampler_dev = RandomUnderSampler(random_state=50) \n",
    "        xs, ys = vect.fit_transform(df_long['Tokens'].values), df_long['Label'].values\n",
    "        x_train, x_dev, y_train, y_dev = train_test_split(xs, ys, train_size=.85,\n",
    "                                                          random_state=42, stratify=ys)\n",
    "        x_dev, y_dev = sampler_dev.fit_resample(x_dev, y_dev)\n",
    "        return x_dev, y_dev    \n",
    "\n",
    "\n",
    "        \n",
    "def make_scaled_dataset(df, stopwords=True, tfidf=True, train=True):\n",
    "    '''Same as above function except with added functionality of StandardScalar'''\n",
    "    if stopwords and tfidf and train:\n",
    "        tknzr = TweetTokenizer(preserve_case=True, reduce_len=True)\n",
    "        vect = TfidfVectorizer(max_df=.9, min_df=25, strip_accents='unicode',\n",
    "                               tokenizer=tknzr.tokenize, stop_words=ALL_STOP_WORDS)\n",
    "        sampler_train = RandomUnderSampler(random_state=42) #undersampling  object to rebalance dataset\n",
    "        scalar = StandardScaler(with_mean=False)\n",
    "        xs, ys = scalar.fit_transform(vect.fit_transform(df['Tokens'].values)), df['Label'].values\n",
    "        x_train, x_dev, y_train, y_dev = train_test_split(xs, ys, train_size=.85, #creating train and dev sets with stratified sampling\n",
    "                                                          random_state=42, stratify=ys) \n",
    "        x_train, y_train = sampler_train.fit_resample(x_train, y_train) \n",
    "        return x_train, y_train\n",
    "    elif stopwords and tfidf and not train:\n",
    "        tknzr = TweetTokenizer(preserve_case=True, reduce_len=True)\n",
    "        vect = TfidfVectorizer(max_df=.9, min_df=25, strip_accents='unicode',\n",
    "                               tokenizer=tknzr.tokenize, stop_words=ALL_STOP_WORDS)\n",
    "        sampler_train = RandomUnderSampler(random_state=42) #undersampling  object to rebalance dataset\n",
    "        scalar = StandardScaler(with_mean=False)\n",
    "        xs, ys = scalar.fit_transform(vect.fit_transform(df['Tokens'].values)), df['Label'].values\n",
    "        x_train, x_dev, y_train, y_dev = train_test_split(xs, ys, train_size=.85, #creating train and dev sets with stratified sampling\n",
    "                                                          random_state=42, stratify=ys) \n",
    "        x_dev, y_dev = sampler_train.fit_resample(x_dev, y_dev) \n",
    "        return x_dev, y_dev\n",
    "    elif not stopwords and tfidf and train:\n",
    "        tknzr = TweetTokenizer(preserve_case=True, reduce_len=True)\n",
    "        vect = TfidfVectorizer(max_df=.9, min_df=25, strip_accents='unicode',\n",
    "                               tokenizer=tknzr.tokenize)\n",
    "        sampler_train = RandomUnderSampler(random_state=42) #undersampling  object to rebalance dataset\n",
    "        scalar = StandardScaler(with_mean=False)\n",
    "        xs, ys = scalar.fit_transform(vect.fit_transform(df['Tokens'].values)), df['Label'].values\n",
    "        x_train, x_dev, y_train, y_dev = train_test_split(xs, ys, train_size=.85, #creating train and dev sets with stratified sampling\n",
    "                                                          random_state=42, stratify=ys) \n",
    "        x_train, y_train = sampler_train.fit_resample(x_train, y_train) \n",
    "        return x_train, y_train\n",
    "    elif not stopwords and tfidf and not train:\n",
    "        tknzr = TweetTokenizer(preserve_case=True, reduce_len=True)\n",
    "        vect = TfidfVectorizer(max_df=.9, min_df=25, strip_accents='unicode',\n",
    "                               tokenizer=tknzr.tokenize)\n",
    "        sampler_train = RandomUnderSampler(random_state=42) #undersampling  object to rebalance dataset\n",
    "        scalar = StandardScaler(with_mean=False)\n",
    "        xs, ys = scalar.fit_transform(vect.fit_transform(df['Tokens'].values)), df['Label'].values\n",
    "        x_train, x_dev, y_train, y_dev = train_test_split(xs, ys, train_size=.85, #creating train and dev sets with stratified sampling\n",
    "                                                          random_state=42, stratify=ys) \n",
    "        x_dev, y_dev = sampler_train.fit_resample(x_dev, y_dev) \n",
    "        return x_dev, y_dev\n",
    "    elif stopwords and not tfidf and train:\n",
    "        tknzr = TweetTokenizer(preserve_case=True, reduce_len=True)\n",
    "        vect = CountVectorizer(max_df=.9, min_df=25, strip_accents='unicode',\n",
    "                               tokenizer=tknzr.tokenize, stop_words=ALL_STOP_WORDS)\n",
    "        sampler_train = RandomUnderSampler(random_state=42) #undersampling  object to rebalance dataset\n",
    "        scalar = StandardScaler(with_mean=False)\n",
    "        xs, ys = scalar.fit_transform(vect.fit_transform(df['Tokens'].values)), df['Label'].values\n",
    "        x_train, x_dev, y_train, y_dev = train_test_split(xs, ys, train_size=.85, #creating train and dev sets with stratified sampling\n",
    "                                                          random_state=42, stratify=ys) \n",
    "        x_train, y_train = sampler_train.fit_resample(x_train, y_train) \n",
    "        return x_train, y_train\n",
    "    elif stopwords and not tfidf and not train:\n",
    "        tknzr = TweetTokenizer(preserve_case=True, reduce_len=True)\n",
    "        vect = CountVectorizer(max_df=.9, min_df=25, strip_accents='unicode',\n",
    "                               tokenizer=tknzr.tokenize, stop_words=ALL_STOP_WORDS)\n",
    "        sampler_train = RandomUnderSampler(random_state=42) #undersampling  object to rebalance dataset\n",
    "        scalar = StandardScaler(with_mean=False)\n",
    "        xs, ys = scalar.fit_transform(vect.fit_transform(df['Tokens'].values)), df['Label'].values\n",
    "        x_train, x_dev, y_train, y_dev = train_test_split(xs, ys, train_size=.85, #creating train and dev sets with stratified sampling\n",
    "                                                          random_state=42, stratify=ys) \n",
    "        x_dev, y_dev = sampler_train.fit_resample(x_dev, y_dev) \n",
    "        return x_dev, y_dev\n",
    "    elif not stopwords and not tfidf and train:\n",
    "        tknzr = TweetTokenizer(preserve_case=True, reduce_len=True)\n",
    "        vect = CountVectorizer(max_df=.9, min_df=25, strip_accents='unicode',\n",
    "                               tokenizer=tknzr.tokenize)\n",
    "        sampler_train = RandomUnderSampler(random_state=42) #undersampling  object to rebalance dataset\n",
    "        scalar = StandardScaler(with_mean=False)\n",
    "        xs, ys = scalar.fit_transform(vect.fit_transform(df['Tokens'].values)), df['Label'].values\n",
    "        x_train, x_dev, y_train, y_dev = train_test_split(xs, ys, train_size=.85, #creating train and dev sets with stratified sampling\n",
    "                                                          random_state=42, stratify=ys) \n",
    "        x_train, y_train = sampler_train.fit_resample(x_train, y_train) \n",
    "        return x_train, y_train\n",
    "    elif not stopwords and not tfidf and not train:\n",
    "        tknzr = TweetTokenizer(preserve_case=True, reduce_len=True)\n",
    "        vect = CountVectorizer(max_df=.9, min_df=25, strip_accents='unicode',\n",
    "                               tokenizer=tknzr.tokenize)\n",
    "        sampler_train = RandomUnderSampler(random_state=42) #undersampling  object to rebalance dataset\n",
    "        scalar = StandardScaler(with_mean=False)\n",
    "        xs, ys = scalar.fit_transform(vect.fit_transform(df['Tokens'].values)), df['Label'].values\n",
    "        x_train, x_dev, y_train, y_dev = train_test_split(xs, ys, train_size=.85, #creating train and dev sets with stratified sampling\n",
    "                                                          random_state=42, stratify=ys) \n",
    "        x_dev, y_dev = sampler_train.fit_resample(x_dev, y_dev) \n",
    "        return x_dev, y_dev\n",
    "\n",
    "def make_dataset_iterators(df):\n",
    "    \"\"\"Creates a tuple of generators. First index holds all train sets while second holds all dev sets.\n",
    "        Makes running the multiple datasets through a model more convenient \"\"\"\n",
    "    all_param_train = make_dataset(df)  # non scaled variation of the dataset\n",
    "    # dev set counterpart of above\n",
    "    all_param_dev = make_dataset(df, train=False)\n",
    "    stopwords_no_tfidf_train = make_dataset(df, tfidf=False)\n",
    "    stopwords_no_tfidf_dev = make_dataset(\n",
    "        df, tfidf=False, train=False)  # devset counterpart of above\n",
    "    no_stop_train = make_dataset(df, stopwords=False)\n",
    "    no_stop_dev = make_dataset(df, stopwords=False, train=False)\n",
    "    no_stopwords_no_tfidf_train = make_dataset(\n",
    "        df, stopwords=False, tfidf=False)\n",
    "    no_stopwords_no_tfidf_dev = make_dataset(\n",
    "        df, stopwords=False, tfidf=False, train=False)\n",
    "\n",
    "    scaled_all_param_train = make_scaled_dataset(df) \n",
    "    scaled_all_param_dev = make_scaled_dataset(df, train=False)\n",
    "    scaled_stop_no_tfidf_train = make_scaled_dataset(df, tfidf=False)\n",
    "    scaled_stop_no_tfidf_dev = make_scaled_dataset(\n",
    "        df, tfidf=False, train=False)\n",
    "    scaled_no_stop_tfidf_train = make_scaled_dataset(df, stopwords=False)\n",
    "    scaled_no_stop_tfidf_dev = make_dataset(df, stopwords=False, train=False)\n",
    "    scaled_no_stop_no_tfidf_train = make_dataset(\n",
    "        df, stopwords=False, tfidf=False)\n",
    "    scaled_no_stop_no_tfidf_dev = make_dataset(\n",
    "        df, stopwords=False, tfidf=False, train=False)\n",
    "\n",
    "    list_train_sets = [all_param_train, stopwords_no_tfidf_train, no_stop_train, no_stopwords_no_tfidf_train, scaled_all_param_train,       #make lists of previously created train                                                                                                                                                  datasets\n",
    "                       scaled_stop_no_tfidf_train, scaled_no_stop_no_tfidf_train, scaled_no_stop_tfidf_train, scaled_no_stop_no_tfidf_train] # make lists of previously created dev sets\n",
    "    list_dev_sets = [all_param_dev, stopwords_no_tfidf_dev, no_stop_dev, no_stopwords_no_tfidf_dev, scaled_all_param_dev,\n",
    "                     scaled_stop_no_tfidf_dev, scaled_no_stop_no_tfidf_dev, scaled_no_stop_tfidf_dev, scaled_no_stop_no_tfidf_dev]\n",
    "\n",
    "    train_gen, dev_gen = iter(list_train_sets), iter(list_dev_sets) #convert them both to iterators to save space \n",
    "    return train_gen, dev_gen\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amaan\\anaconda3\\envs\\t_mining\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['&', \"'\", 'n', '‘', '’'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\amaan\\anaconda3\\envs\\t_mining\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['&', \"'\", 'n', '‘', '’'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\amaan\\anaconda3\\envs\\t_mining\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['&', \"'\", 'n', '‘', '’'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\amaan\\anaconda3\\envs\\t_mining\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['&', \"'\", 'n', '‘', '’'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\amaan\\anaconda3\\envs\\t_mining\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['&', \"'\", 'n', '‘', '’'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\amaan\\anaconda3\\envs\\t_mining\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['&', \"'\", 'n', '‘', '’'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\amaan\\anaconda3\\envs\\t_mining\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['&', \"'\", 'n', '‘', '’'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\amaan\\anaconda3\\envs\\t_mining\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['&', \"'\", 'n', '‘', '’'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "<list_iterator object at 0x000002026AA0D2C8>\n",
      "<list_iterator object at 0x000002026A68DD88>\n"
     ]
    }
   ],
   "source": [
    "tuple_of_generators = make_dataset_iterators(df_long)\n",
    "print(type(tuple_of_generators))\n",
    "print(tuple_of_generators[0])\n",
    "print(tuple_of_generators[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<2250x1454 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 11944 stored elements in Compressed Sparse Row format>, array([0, 0, 0, ..., 1, 1, 1], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(next(tuple_of_generators[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ClassifierBot():\n",
    "    \n",
    "    def __init__(self, trainset, devset, bot):\n",
    "        self.train = trainset\n",
    "        self.dev = devset\n",
    "        self.bot = bot\n",
    "        \n",
    "    def classify(self):\n",
    "        fitted_bot = self.bot.fit(self.train[0], self.train[1])\n",
    "        prediction = fitted_bot.predict(self.dev[0])\n",
    "        report = classification_report(self.dev[1], prediction, output_dict=True)\n",
    "        print(report) # print statement to easily see the performance of each classifier\n",
    "        return fitted_bot, prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bots(iter_train, iter_dev, clf):\n",
    "    tup_lst, dev_lst = [], [] # lst for storing\n",
    "    for i in range(0, 9): #iterating through all 9 datasets\n",
    "        current_dev_set = next(iter_dev) #storing the devset sequence for reuse with LIME\n",
    "        print(type(current_dev_set))\n",
    "        bot = ClassifierBot(next(iter_train), current_dev_set, clf)\n",
    "        bot_pred_tup = bot.classify()  \n",
    "        #print(type(bot_pred_tup))\n",
    "        tup_lst.append(bot_pred_tup)\n",
    "        dev_lst.append(current_dev_set)\n",
    "    return iter(tup_lst), iter(dev_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "def create_pipeline(clf, dataset):\n",
    "    \"\"\"Function for Pipeline with whatever classifier we want\"\"\"\n",
    "    temp_lst = []\n",
    "    for i in range(0, 2):\n",
    "        tknzr = TweetTokenizer(preserve_case=True, reduce_len=True)\n",
    "        vect = TfidfVectorizer(\n",
    "            max_df=.9, min_df=25, strip_accents='unicode', tokenizer=tknzr.tokenize)\n",
    "        scaler = StandardScaler(with_mean=False)\n",
    "        sampler = RandomUnderSampler(random_state=42)\n",
    "        pipeline = Pipeline([('vect', vect), ('scale', scaler),\n",
    "                             ('sampler', sampler), ('clf', clf)])\n",
    "        param_grid = [{\n",
    "            'vect__stop_words': [None, ALL_STOP_WORDS]\n",
    "        }]\n",
    "        grid_srch = GridSearchCV(\n",
    "            estimator=pipeline, param_grid=param_grid, refit=True, n_jobs=-1)\n",
    "        current_dev_set, current_train_set = next(dataset[1]), next(dataset[0])\n",
    "        grid_srch.fit(current_train_set[0], current_train_set[1])\n",
    "        prediction = grid_srch.predict(current_dev_set[0])\n",
    "        report = classification_report(\n",
    "            current_dev_set[1], prediction, output_dict=True)\n",
    "        best_est = grid_srch.best_estimator_\n",
    "        tup = (best_est, prediction)\n",
    "        temp_lst.append(tup)\n",
    "    return temp_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amaan\\anaconda3\\envs\\t_mining\\lib\\site-packages\\sklearn\\model_selection\\_search.py:921: UserWarning: One or more of the test scores are non-finite: [nan nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-7532057d5e54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtup_sgd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple_of_generators\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-88-bc53438f992b>\u001b[0m in \u001b[0;36mcreate_pipeline\u001b[1;34m(clf, dataset)\u001b[0m\n\u001b[0;32m     21\u001b[0m             estimator=pipeline, param_grid=param_grid, refit=True, n_jobs=-1)\n\u001b[0;32m     22\u001b[0m         \u001b[0mcurrent_dev_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_train_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mgrid_srch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_train_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_train_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_srch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_dev_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         report = classification_report(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\t_mining\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\t_mining\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\t_mining\\lib\\site-packages\\imblearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \"\"\"\n\u001b[0;32m    261\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Pipeline\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"passthrough\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\t_mining\\lib\\site-packages\\imblearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    215\u001b[0m                     \u001b[0mmessage_clsname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Pipeline\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m                     \u001b[0mmessage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                     \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m                 )\n\u001b[0;32m    219\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fit_resample\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\t_mining\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\t_mining\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\t_mining\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1848\u001b[0m         \"\"\"\n\u001b[0;32m   1849\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1850\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1851\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\t_mining\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1203\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m-> 1204\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m   1205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1206\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\t_mining\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1115\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1116\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1117\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\t_mining\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\t_mining\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \"\"\"\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\t_mining\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    685\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 687\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" not found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "tup_sgd = create_pipeline(SVC(random_state=42), tuple_of_generators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "{'0': {'precision': 0.7259430331023865, 'recall': 0.8382222222222222, 'f1-score': 0.778052805280528, 'support': 1125}, '1': {'precision': 0.8086225026288117, 'recall': 0.6835555555555556, 'f1-score': 0.7408477842003853, 'support': 1125}, 'accuracy': 0.7608888888888888, 'macro avg': {'precision': 0.7672827678655991, 'recall': 0.760888888888889, 'f1-score': 0.7594502947404567, 'support': 2250}, 'weighted avg': {'precision': 0.7672827678655991, 'recall': 0.7608888888888888, 'f1-score': 0.7594502947404567, 'support': 2250}}\n",
      "<class 'tuple'>\n",
      "{'0': {'precision': 0.7126168224299065, 'recall': 0.8133333333333334, 'f1-score': 0.759651307596513, 'support': 1125}, '1': {'precision': 0.782608695652174, 'recall': 0.672, 'f1-score': 0.7230989956958394, 'support': 1125}, 'accuracy': 0.7426666666666667, 'macro avg': {'precision': 0.7476127590410402, 'recall': 0.7426666666666667, 'f1-score': 0.7413751516461762, 'support': 2250}, 'weighted avg': {'precision': 0.7476127590410403, 'recall': 0.7426666666666667, 'f1-score': 0.7413751516461762, 'support': 2250}}\n",
      "<class 'tuple'>\n",
      "{'0': {'precision': 0.7394888705688376, 'recall': 0.7973333333333333, 'f1-score': 0.7673224978614199, 'support': 1125}, '1': {'precision': 0.7801350048216008, 'recall': 0.7191111111111111, 'f1-score': 0.7483811285846439, 'support': 1125}, 'accuracy': 0.7582222222222222, 'macro avg': {'precision': 0.7598119376952193, 'recall': 0.7582222222222222, 'f1-score': 0.7578518132230319, 'support': 2250}, 'weighted avg': {'precision': 0.7598119376952192, 'recall': 0.7582222222222222, 'f1-score': 0.7578518132230319, 'support': 2250}}\n",
      "<class 'tuple'>\n",
      "{'0': {'precision': 0.723911257189811, 'recall': 0.7831111111111111, 'f1-score': 0.7523484201537147, 'support': 1125}, '1': {'precision': 0.7637947725072604, 'recall': 0.7013333333333334, 'f1-score': 0.731232622798888, 'support': 1125}, 'accuracy': 0.7422222222222222, 'macro avg': {'precision': 0.7438530148485356, 'recall': 0.7422222222222222, 'f1-score': 0.7417905214763014, 'support': 2250}, 'weighted avg': {'precision': 0.7438530148485357, 'recall': 0.7422222222222222, 'f1-score': 0.7417905214763013, 'support': 2250}}\n",
      "<class 'tuple'>\n",
      "{'0': {'precision': 0.7001718213058419, 'recall': 0.7244444444444444, 'f1-score': 0.7121013543031892, 'support': 1125}, '1': {'precision': 0.714548802946593, 'recall': 0.6897777777777778, 'f1-score': 0.7019448213478064, 'support': 1125}, 'accuracy': 0.7071111111111111, 'macro avg': {'precision': 0.7073603121262174, 'recall': 0.7071111111111111, 'f1-score': 0.7070230878254978, 'support': 2250}, 'weighted avg': {'precision': 0.7073603121262173, 'recall': 0.7071111111111111, 'f1-score': 0.7070230878254978, 'support': 2250}}\n",
      "<class 'tuple'>\n",
      "{'0': {'precision': 0.6995033112582781, 'recall': 0.7511111111111111, 'f1-score': 0.7243891984569224, 'support': 1125}, '1': {'precision': 0.7312859884836852, 'recall': 0.6773333333333333, 'f1-score': 0.7032764190124595, 'support': 1125}, 'accuracy': 0.7142222222222222, 'macro avg': {'precision': 0.7153946498709817, 'recall': 0.7142222222222222, 'f1-score': 0.713832808734691, 'support': 2250}, 'weighted avg': {'precision': 0.7153946498709817, 'recall': 0.7142222222222222, 'f1-score': 0.7138328087346909, 'support': 2250}}\n",
      "<class 'tuple'>\n",
      "{'0': {'precision': 0.723911257189811, 'recall': 0.7831111111111111, 'f1-score': 0.7523484201537147, 'support': 1125}, '1': {'precision': 0.7637947725072604, 'recall': 0.7013333333333334, 'f1-score': 0.731232622798888, 'support': 1125}, 'accuracy': 0.7422222222222222, 'macro avg': {'precision': 0.7438530148485356, 'recall': 0.7422222222222222, 'f1-score': 0.7417905214763014, 'support': 2250}, 'weighted avg': {'precision': 0.7438530148485357, 'recall': 0.7422222222222222, 'f1-score': 0.7417905214763013, 'support': 2250}}\n",
      "<class 'tuple'>\n",
      "{'0': {'precision': 0.5, 'recall': 1.0, 'f1-score': 0.6666666666666666, 'support': 1125}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1125}, 'accuracy': 0.5, 'macro avg': {'precision': 0.25, 'recall': 0.5, 'f1-score': 0.3333333333333333, 'support': 2250}, 'weighted avg': {'precision': 0.25, 'recall': 0.5, 'f1-score': 0.3333333333333333, 'support': 2250}}\n",
      "<class 'tuple'>\n",
      "{'0': {'precision': 0.723911257189811, 'recall': 0.7831111111111111, 'f1-score': 0.7523484201537147, 'support': 1125}, '1': {'precision': 0.7637947725072604, 'recall': 0.7013333333333334, 'f1-score': 0.731232622798888, 'support': 1125}, 'accuracy': 0.7422222222222222, 'macro avg': {'precision': 0.7438530148485356, 'recall': 0.7422222222222222, 'f1-score': 0.7417905214763014, 'support': 2250}, 'weighted avg': {'precision': 0.7438530148485357, 'recall': 0.7422222222222222, 'f1-score': 0.7417905214763013, 'support': 2250}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amaan\\anaconda3\\envs\\t_mining\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\amaan\\anaconda3\\envs\\t_mining\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\amaan\\anaconda3\\envs\\t_mining\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "tup_bots_sgd = bots(tuple_of_generators[0], tuple_of_generators[1], SGDClassifier(random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "<class 'list_iterator'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "#Quickly Verifying whether we are accessing the correct objects\n",
    "\n",
    "print(type(tup_bots_sgd))\n",
    "print(type(tup_bots_sgd[1]))\n",
    "for i in range(0, 2):\n",
    "    print(type(next(tup_bots_sgd[1])))\n",
    "#for i in range(0, 2):\n",
    "#    tup = next(tup_bots_sgd)\n",
    "#    print(type(tup[0]), type(tup[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Neural Network Classifier \n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tup_bots_MLP = bots(tuple_of_generators[0], tuple_of_generators[1], MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(10,30,10,5), random_state=42, batch_size=128, max_iter=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 0.7183863460046548, 'recall': 0.8231111111111111, 'f1-score': 0.767191383595692, 'support': 1125}, '1': {'precision': 0.7929240374609782, 'recall': 0.6773333333333333, 'f1-score': 0.7305848513902207, 'support': 1125}, 'accuracy': 0.7502222222222222, 'macro avg': {'precision': 0.7556551917328165, 'recall': 0.7502222222222222, 'f1-score': 0.7488881174929563, 'support': 2250}, 'weighted avg': {'precision': 0.7556551917328165, 'recall': 0.7502222222222222, 'f1-score': 0.7488881174929563, 'support': 2250}}\n",
      "<class 'dict'>\n",
      "{'0': {'precision': 0.7123893805309734, 'recall': 0.8586666666666667, 'f1-score': 0.7787182587666264, 'support': 1125}, '1': {'precision': 0.8221476510067114, 'recall': 0.6533333333333333, 'f1-score': 0.7280832095096582, 'support': 1125}, 'accuracy': 0.756, 'macro avg': {'precision': 0.7672685157688424, 'recall': 0.756, 'f1-score': 0.7534007341381423, 'support': 2250}, 'weighted avg': {'precision': 0.7672685157688424, 'recall': 0.756, 'f1-score': 0.7534007341381422, 'support': 2250}}\n",
      "<class 'dict'>\n",
      "{'0': {'precision': 0.7389558232931727, 'recall': 0.8177777777777778, 'f1-score': 0.7763713080168776, 'support': 1125}, '1': {'precision': 0.7960199004975125, 'recall': 0.7111111111111111, 'f1-score': 0.7511737089201879, 'support': 1125}, 'accuracy': 0.7644444444444445, 'macro avg': {'precision': 0.7674878618953426, 'recall': 0.7644444444444445, 'f1-score': 0.7637725084685327, 'support': 2250}, 'weighted avg': {'precision': 0.7674878618953426, 'recall': 0.7644444444444445, 'f1-score': 0.7637725084685327, 'support': 2250}}\n",
      "<class 'dict'>\n",
      "{'0': {'precision': 0.7105064247921391, 'recall': 0.8355555555555556, 'f1-score': 0.7679738562091503, 'support': 1125}, '1': {'precision': 0.8004314994606256, 'recall': 0.6595555555555556, 'f1-score': 0.7231968810916178, 'support': 1125}, 'accuracy': 0.7475555555555555, 'macro avg': {'precision': 0.7554689621263824, 'recall': 0.7475555555555555, 'f1-score': 0.745585368650384, 'support': 2250}, 'weighted avg': {'precision': 0.7554689621263824, 'recall': 0.7475555555555555, 'f1-score': 0.745585368650384, 'support': 2250}}\n",
      "<class 'dict'>\n",
      "{'0': {'precision': 0.7109872611464968, 'recall': 0.7937777777777778, 'f1-score': 0.750104997900042, 'support': 1125}, '1': {'precision': 0.7665995975855131, 'recall': 0.6773333333333333, 'f1-score': 0.7192071731949032, 'support': 1125}, 'accuracy': 0.7355555555555555, 'macro avg': {'precision': 0.738793429366005, 'recall': 0.7355555555555555, 'f1-score': 0.7346560855474726, 'support': 2250}, 'weighted avg': {'precision': 0.7387934293660049, 'recall': 0.7355555555555555, 'f1-score': 0.7346560855474726, 'support': 2250}}\n",
      "<class 'dict'>\n",
      "{'0': {'precision': 0.702962962962963, 'recall': 0.8435555555555555, 'f1-score': 0.7668686868686868, 'support': 1125}, '1': {'precision': 0.8044444444444444, 'recall': 0.6435555555555555, 'f1-score': 0.7150617283950617, 'support': 1125}, 'accuracy': 0.7435555555555555, 'macro avg': {'precision': 0.7537037037037038, 'recall': 0.7435555555555555, 'f1-score': 0.7409652076318742, 'support': 2250}, 'weighted avg': {'precision': 0.7537037037037038, 'recall': 0.7435555555555555, 'f1-score': 0.7409652076318741, 'support': 2250}}\n",
      "<class 'dict'>\n",
      "{'0': {'precision': 0.7105064247921391, 'recall': 0.8355555555555556, 'f1-score': 0.7679738562091503, 'support': 1125}, '1': {'precision': 0.8004314994606256, 'recall': 0.6595555555555556, 'f1-score': 0.7231968810916178, 'support': 1125}, 'accuracy': 0.7475555555555555, 'macro avg': {'precision': 0.7554689621263824, 'recall': 0.7475555555555555, 'f1-score': 0.745585368650384, 'support': 2250}, 'weighted avg': {'precision': 0.7554689621263824, 'recall': 0.7475555555555555, 'f1-score': 0.745585368650384, 'support': 2250}}\n",
      "<class 'dict'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chengyiyang/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 0.5, 'recall': 1.0, 'f1-score': 0.6666666666666666, 'support': 1125}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1125}, 'accuracy': 0.5, 'macro avg': {'precision': 0.25, 'recall': 0.5, 'f1-score': 0.3333333333333333, 'support': 2250}, 'weighted avg': {'precision': 0.25, 'recall': 0.5, 'f1-score': 0.3333333333333333, 'support': 2250}}\n",
      "<class 'dict'>\n",
      "{'0': {'precision': 0.7105064247921391, 'recall': 0.8355555555555556, 'f1-score': 0.7679738562091503, 'support': 1125}, '1': {'precision': 0.8004314994606256, 'recall': 0.6595555555555556, 'f1-score': 0.7231968810916178, 'support': 1125}, 'accuracy': 0.7475555555555555, 'macro avg': {'precision': 0.7554689621263824, 'recall': 0.7475555555555555, 'f1-score': 0.745585368650384, 'support': 2250}, 'weighted avg': {'precision': 0.7554689621263824, 'recall': 0.7475555555555555, 'f1-score': 0.745585368650384, 'support': 2250}}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "#Support Vector Classifier without Gridsearch\n",
    "\n",
    "\n",
    "#tup_bots_svc = bots(tuple_of_generators[0], tuple_of_generators[1], SVC())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 0.7456896551724138, 'recall': 0.7688888888888888, 'f1-score': 0.7571115973741794, 'support': 1125}, '1': {'precision': 0.7614678899082569, 'recall': 0.7377777777777778, 'f1-score': 0.7494356659142213, 'support': 1125}, 'accuracy': 0.7533333333333333, 'macro avg': {'precision': 0.7535787725403353, 'recall': 0.7533333333333333, 'f1-score': 0.7532736316442004, 'support': 2250}, 'weighted avg': {'precision': 0.7535787725403355, 'recall': 0.7533333333333333, 'f1-score': 0.7532736316442004, 'support': 2250}}\n",
      "<class 'dict'>\n",
      "{'0': {'precision': 0.73502722323049, 'recall': 0.72, 'f1-score': 0.7274360125729681, 'support': 1125}, '1': {'precision': 0.725609756097561, 'recall': 0.7404444444444445, 'f1-score': 0.7329520457545093, 'support': 1125}, 'accuracy': 0.7302222222222222, 'macro avg': {'precision': 0.7303184896640255, 'recall': 0.7302222222222222, 'f1-score': 0.7301940291637388, 'support': 2250}, 'weighted avg': {'precision': 0.7303184896640255, 'recall': 0.7302222222222222, 'f1-score': 0.7301940291637387, 'support': 2250}}\n",
      "<class 'dict'>\n",
      "{'0': {'precision': 0.7317275747508306, 'recall': 0.7831111111111111, 'f1-score': 0.7565478746243024, 'support': 1125}, '1': {'precision': 0.7667304015296367, 'recall': 0.7128888888888889, 'f1-score': 0.7388300322432058, 'support': 1125}, 'accuracy': 0.748, 'macro avg': {'precision': 0.7492289881402336, 'recall': 0.748, 'f1-score': 0.7476889534337541, 'support': 2250}, 'weighted avg': {'precision': 0.7492289881402336, 'recall': 0.748, 'f1-score': 0.7476889534337542, 'support': 2250}}\n",
      "<class 'dict'>\n",
      "{'0': {'precision': 0.7425828970331588, 'recall': 0.7564444444444445, 'f1-score': 0.7494495816820784, 'support': 1125}, '1': {'precision': 0.7518115942028986, 'recall': 0.7377777777777778, 'f1-score': 0.7447285778375953, 'support': 1125}, 'accuracy': 0.7471111111111111, 'macro avg': {'precision': 0.7471972456180287, 'recall': 0.7471111111111111, 'f1-score': 0.7470890797598368, 'support': 2250}, 'weighted avg': {'precision': 0.7471972456180287, 'recall': 0.7471111111111111, 'f1-score': 0.7470890797598368, 'support': 2250}}\n",
      "<class 'dict'>\n",
      "{'0': {'precision': 0.7461273666092944, 'recall': 0.7706666666666667, 'f1-score': 0.7581985133362484, 'support': 1125}, '1': {'precision': 0.7628676470588235, 'recall': 0.7377777777777778, 'f1-score': 0.7501129688206055, 'support': 1125}, 'accuracy': 0.7542222222222222, 'macro avg': {'precision': 0.7544975068340589, 'recall': 0.7542222222222222, 'f1-score': 0.754155741078427, 'support': 2250}, 'weighted avg': {'precision': 0.7544975068340589, 'recall': 0.7542222222222222, 'f1-score': 0.754155741078427, 'support': 2250}}\n",
      "<class 'dict'>\n",
      "{'0': {'precision': 0.7321100917431193, 'recall': 0.7093333333333334, 'f1-score': 0.7205417607223475, 'support': 1125}, '1': {'precision': 0.718103448275862, 'recall': 0.7404444444444445, 'f1-score': 0.7291028446389497, 'support': 1125}, 'accuracy': 0.7248888888888889, 'macro avg': {'precision': 0.7251067700094906, 'recall': 0.7248888888888889, 'f1-score': 0.7248223026806486, 'support': 2250}, 'weighted avg': {'precision': 0.7251067700094906, 'recall': 0.7248888888888889, 'f1-score': 0.7248223026806486, 'support': 2250}}\n",
      "<class 'dict'>\n",
      "{'0': {'precision': 0.7425828970331588, 'recall': 0.7564444444444445, 'f1-score': 0.7494495816820784, 'support': 1125}, '1': {'precision': 0.7518115942028986, 'recall': 0.7377777777777778, 'f1-score': 0.7447285778375953, 'support': 1125}, 'accuracy': 0.7471111111111111, 'macro avg': {'precision': 0.7471972456180287, 'recall': 0.7471111111111111, 'f1-score': 0.7470890797598368, 'support': 2250}, 'weighted avg': {'precision': 0.7471972456180287, 'recall': 0.7471111111111111, 'f1-score': 0.7470890797598368, 'support': 2250}}\n",
      "<class 'dict'>\n",
      "{'0': {'precision': 0.5014985014985015, 'recall': 0.8924444444444445, 'f1-score': 0.6421490246242405, 'support': 1125}, '1': {'precision': 0.5120967741935484, 'recall': 0.11288888888888889, 'f1-score': 0.18499635833940276, 'support': 1125}, 'accuracy': 0.5026666666666667, 'macro avg': {'precision': 0.5067976378460249, 'recall': 0.5026666666666667, 'f1-score': 0.41357269148182163, 'support': 2250}, 'weighted avg': {'precision': 0.5067976378460249, 'recall': 0.5026666666666667, 'f1-score': 0.4135726914818216, 'support': 2250}}\n",
      "<class 'dict'>\n",
      "{'0': {'precision': 0.7425828970331588, 'recall': 0.7564444444444445, 'f1-score': 0.7494495816820784, 'support': 1125}, '1': {'precision': 0.7518115942028986, 'recall': 0.7377777777777778, 'f1-score': 0.7447285778375953, 'support': 1125}, 'accuracy': 0.7471111111111111, 'macro avg': {'precision': 0.7471972456180287, 'recall': 0.7471111111111111, 'f1-score': 0.7470890797598368, 'support': 2250}, 'weighted avg': {'precision': 0.7471972456180287, 'recall': 0.7471111111111111, 'f1-score': 0.7470890797598368, 'support': 2250}}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "#RandomForestClassifier without Gridsearch\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "tup_bots_forest = bots(tuple_of_generators[0], tuple_of_generators[1], RandomForestClassifier(n_jobs=-1, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################## LIME ###################\n",
    "Need explainer object, instance of test data = tuple_of_generators[1][whichever dataset performed best], instance of the classifier rf = pull from list tup_bots_forst[0][whichever dataset peformed best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "class_names = ['Hateful', 'Non-Hateful']\n",
    "exp = LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "def best_val_set(tup, r):\n",
    "    pass\n",
    "\n",
    "def best_clf(tup, r):\n",
    "    \"\"\"Input tuple of (classifiers_generator, predictions_generator) and index \n",
    "    of classifier to extract from tuple -> outputs desired classifier\"\"\"\n",
    "    lst_of_classifiers = [] # empty list for storing\n",
    "    for i in range(0, r): # iteration for looping through sequence\n",
    "        classifier = next(tup[0]) # access each classifier object\n",
    "        lst_of_classifiers.append(classifier) #append it to empty list\n",
    "    return lst_of_classifiers.pop() #pop out the final entry to the list - desired classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.ClassifierBot object at 0x0000020267B36A88>\n"
     ]
    }
   ],
   "source": [
    "best_rf = best_clf(tup_bots_forest, 1) #best performing classifier\n",
    "print(best_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<list_iterator object at 0x0000020267C8BB88>\n"
     ]
    }
   ],
   "source": [
    "print(tup_bots_forest[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hi(r):\n",
    "    lst = [1, 2, 3, 4, 5, 6, 7]\n",
    "    print(lst[0])\n",
    "    new_lst = []\n",
    "    for i in range(0, r):\n",
    "        clf = lst.pop()\n",
    "        new_lst.append(clf)\n",
    "    return new_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[7, 6, 5]\n",
      "1\n",
      "[]\n",
      "1\n",
      "[7]\n"
     ]
    }
   ],
   "source": [
    "print(hi(3))\n",
    "print(hi(0))\n",
    "print(hi(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#classification report\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,recall_score,f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def report(prediction, real_labels, data):\n",
    "    print(\"classification report as follows: \")\n",
    "    print(f'   Accuracy: {accuracy_score(prediction, real_labels)}')\n",
    "    print(f'   Precision: {precision_score(prediction, real_labels,average=\"macro\")}')\n",
    "    print(f'   recall: {recall_score(prediction, real_labels,average=\"macro\")}')\n",
    "    print(f'   F1 measure: {f1_score(prediction, real_labels,average=\"macro\")}')\n",
    "    print('Show 5 example of correctly classified datapoint: ')\n",
    "    if data[prediction==real_labels].shape[0] > 5:\n",
    "        display(data[prediction==real_labels].iloc[:5,:])\n",
    "    else:\n",
    "        display(data[prediction==real_labels])\n",
    "    print('Show 5 example of wrongly classified datapoint: ')\n",
    "    if data[prediction!=real_labels].shape[0] > 5:\n",
    "        display(data[prediction!=real_labels].iloc[:5,:])\n",
    "    else:\n",
    "        display(data[prediction!=real_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report as follows: \n",
      "   Accuracy: 0.8168943476626144\n",
      "   Precision: 0.7084592624109877\n",
      "   recall: 0.7811589138333501\n",
      "   F1 measure: 0.7309291098045785\n",
      "Show 5 example of correctly classified datapoint: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23121</th>\n",
       "      <td>rt strategic vote kat food truly awful #mkr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35928</th>\n",
       "      <td>pancakes proof deity love us #mkr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45316</th>\n",
       "      <td>sick see fuck asshole bitch make chain latters...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42230</th>\n",
       "      <td>i'm try get insight trans issue definitely gro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40128</th>\n",
       "      <td>make</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tokens  Label\n",
       "23121        rt strategic vote kat food truly awful #mkr      0\n",
       "35928                  pancakes proof deity love us #mkr      0\n",
       "45316  sick see fuck asshole bitch make chain latters...      1\n",
       "42230  i'm try get insight trans issue definitely gro...      0\n",
       "40128                                               make      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show 5 example of wrongly classified datapoint: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48658</th>\n",
       "      <td>lol ralph guy still moi era</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30437</th>\n",
       "      <td>kill muslims oppose kill ezidis christians non...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28396</th>\n",
       "      <td>single men cannot adopt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44924</th>\n",
       "      <td>muslim brotherhood usa hundred years liken say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7346</th>\n",
       "      <td>rt #mosul christian pastor #paul_jacob sentenc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tokens  Label\n",
       "48658                        lol ralph guy still moi era      1\n",
       "30437  kill muslims oppose kill ezidis christians non...      0\n",
       "28396                            single men cannot adopt      1\n",
       "44924  muslim brotherhood usa hundred years liken say...      0\n",
       "7346   rt #mosul christian pastor #paul_jacob sentenc...      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_x = vectorizer.transform(validation_set['Tokens'])\n",
    "val_x = transformer.transform(val_x)\n",
    "\n",
    "predict = ntwk.predict(val_x)\n",
    "report(predict, validation_set['label'], validation_set[['Tokens','Label']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
