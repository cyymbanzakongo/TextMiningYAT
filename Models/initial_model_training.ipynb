{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  #Imports for converting dataframe to train_dev splits\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import nltk, spacy\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import STOPWORDS\n",
    "from collections import Counter\n",
    "from gensim.parsing.preprocessing import STOPWORDS as SW\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "pd.options.display.float_format = '{:,.0f}'.format\n",
    "in_file = Path.cwd().parents[0] / 'Processed_datasets' / 'final_db.csv'\n",
    "my_stopwords = stopwords.words('english')\n",
    "stopwords = set(STOPWORDS).union(my_stopwords) #preparing stopwards list\n",
    "custom_stopwords = ['hi', '\\n', '\\n\\n', '&amp;', ' ', '.', '-',\n",
    "                    'got', \"it's\", 'it’s', \"i'm\", 'i’m', 'im', 'want', 'like', '$', '@']\n",
    "nlp = spacy.load('en_core_web_sm', exclude=['ner', 'tok2vec', 'tagger', 'paerser', 'senter', 'lemmatizer', 'attribute_ruler']) # using only for stopwords\n",
    "STOP_WORDS = nlp.Defaults.stop_words.union(custom_stopwords)\n",
    "ALL_STOP_WORDS = STOP_WORDS.union(SW).union(stopwords)\n",
    "CHUNK_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = pd.read_csv(in_file, engine='python', usecols=['Text', 'oh_label'], encoding='utf-8') #not using unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Text  oh_label\n",
      "0   @AAlwuhaib1977 Muslim mob violence against Hin...         1\n",
      "1              @Te4m_NiGhtM4Re http://t.co/5Ih7MkDbQG         0\n",
      "2   @jncatron @isra_jourisra @AMPalestine Islamoph...         1\n",
      "3   Finally I'm all caught up, and that sudden dea...         0\n",
      "4              @carolinesinders @herecomesfran *hugs*         0\n",
      "5   Please, PLEASE start using \"is your discernmen...         0\n",
      "6   @aymannathem As soon as ISIS chased all the mi...         0\n",
      "7   @Ali_Gharib @MaxBlumenthal Glad you like it. h...         0\n",
      "8   @HuffPostRelig Islam invaded and conquered 2/3...         1\n",
      "9   @semzyxx Do you approve of your pedophile prop...         1\n",
      "10  @watan71969 @geeky_zekey Problem with vile Mus...         1\n",
      "11  @Skawtnyc @athenahollow @twoscooters i don't t...         0\n",
      "12  @dylanw that's cool. next time when a woman ta...         0\n",
      "13  RT @hadi_elis: Erdogan's Egyptian Nightmare \\n...         0\n",
      "14  RT @mykitchenrules: Our judges are about to tu...         0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text        25596\n",
       "oh_label    25596\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_long.head(15))\n",
    "df_long.count()\n",
    "#print(df_long['Text'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#setting up baseline pipeline\n",
    "from nltk.tokenize import TweetTokenizer #I chose to tokenize with this, as it gets rid of @ handlers\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #easy idf and stopword removal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler #moderately imbalanced dataset so randomundersampling - approx 2:1 ratio\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC, SVC, SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "PARAM_GRID = [{\n",
    "        'vect__stop_words': [None, ALL_STOP_WORDS],\n",
    "        #'clf__kernel' : ['sigmoid', 'rbf'],\n",
    "        #'clf__loss' : ['hinge', 'squared_hinge'],\n",
    "       # 'clf__activation' : ['tanh', 'relu', 'logistic'],\n",
    "        #'clf__hidden_layer_sizes' : [(10, 20, 30), (15, 25, 10), (40, 30, 25)],\n",
    "        'vect__use_idf' : [True, False]\n",
    "    }]\n",
    "\n",
    "\n",
    "def make_pipeline(df, clf, params, standardise=True):\n",
    "    if standardise == True:\n",
    "        xs, ys = df['Text'].values, df['oh_label'].values\n",
    "        x_train, x_dev, y_train, y_dev = train_test_split(\n",
    "            xs, ys, train_size=.85, random_state=42, stratify=ys)\n",
    "        tknzr = TweetTokenizer(preserve_case=True, reduce_len=True)\n",
    "        vect = TfidfVectorizer(\n",
    "            max_df=.9, min_df=25, strip_accents='unicode', tokenizer=tknzr.tokenize)\n",
    "        scaler = StandardScaler(with_mean=False)\n",
    "        sampler = RandomUnderSampler(random_state=42)\n",
    "        pipeline = Pipeline([('vect', vect), ('scale', scaler),\n",
    "                             ('sampler', sampler), ('clf', clf)])\n",
    "        grid_srch = GridSearchCV(\n",
    "            estimator=pipeline, param_grid=params, refit=True, n_jobs=-1)\n",
    "        grid_srch.fit(x_train, y_train)  # fit the grid_search object\n",
    "        prediction = grid_srch.predict(x_dev)  # Obtain predictions and save them\n",
    "        # obtain classification report of preds\n",
    "        report = classification_report(y_dev, prediction, output_dict=True)\n",
    "        best_est = grid_srch.best_estimator_\n",
    "        print(report)\n",
    "        return best_est, prediction, x_dev, y_dev\n",
    "    xs, ys = df['Text'].values, df['oh_label'].values\n",
    "    x_train, x_dev, y_train, y_dev = train_test_split(\n",
    "         xs, ys, train_size=.85, random_state=42, stratify=ys)\n",
    "    tknzr = TweetTokenizer(preserve_case=True, reduce_len=True)\n",
    "    vect = TfidfVectorizer(max_df=.9, min_df=25, strip_accents='unicode', tokenizer=tknzr.tokenize)\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "    sampler = RandomUnderSampler(random_state=42)\n",
    "    pipeline = Pipeline([('vect', vect), ('scale', scaler),\n",
    "                             ('sampler', sampler), ('clf', clf)])\n",
    "    grid_srch = GridSearchCV(\n",
    "        estimator=pipeline, param_grid=params, refit=True, n_jobs=-1)\n",
    "    grid_srch.fit(x_train, y_train)  # fit the grid_search object\n",
    "    prediction = grid_srch.predict(x_dev)  # Obtain predictions and save them\n",
    "    # obtain classification report of preds\n",
    "    report = classification_report(y_dev, prediction, output_dict=True)\n",
    "    best_est = grid_srch.best_estimator_\n",
    "    print(report)\n",
    "    return best_est, prediction, x_dev, y_dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chengyiyang/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0.0': {'precision': 0.8696453247351451, 'recall': 0.7203357497138497, 'f1-score': 0.7879799666110184, 'support': 2621}, '1.0': {'precision': 0.560814859197124, 'recall': 0.7678424938474159, 'f1-score': 0.6481994459833795, 'support': 1219}, 'accuracy': 0.7354166666666667, 'macro avg': {'precision': 0.7152300919661345, 'recall': 0.7440891217806328, 'f1-score': 0.7180897062971989, 'support': 3840}, 'weighted avg': {'precision': 0.7716077368469035, 'recall': 0.7354166666666667, 'f1-score': 0.7436069315471925, 'support': 3840}}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "tup_MLP = make_pipeline(df_long, MLPClassifier(random_state=42), PARAM_GRID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Pipeline(steps=[('vect',\n",
       "                  TfidfVectorizer(max_df=0.9, min_df=25, strip_accents='unicode',\n",
       "                                  tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f89f1ca1760>>)),\n",
       "                 ('scale', StandardScaler(with_mean=False)),\n",
       "                 ('sampler', RandomUnderSampler(random_state=42)),\n",
       "                 ('clf',\n",
       "                  MLPClassifier(activation='logistic',\n",
       "                                hidden_layer_sizes=(15, 25, 10),\n",
       "                                random_state=42))]),\n",
       " array([0., 1., 0., ..., 0., 0., 0.]),\n",
       " array(['.@BlackOpal80 Like, if #GamerGate really just hated women, why bother with GG?  Need NO permission to hate. They could just hate/play games.',\n",
       "        '\"@londonmum\\\\xa0 Go out for another cigarette.\\\\xa0 You are having nicotine withdrawal\"',\n",
       "        '\"Respectfully, I think you\\'re wrong Alain. It is genetic. If physiological traits are determined by race, then what is so far-fetched about that notion that psychological/intelligence traits (IQ, future-time orientation, delayed gratification) are also genetically predetermined?\\\\n\\\\nLet\\'s cut the horseshit. 20th Century.....\\\\n\\\\nWASPs = civilization, art, literature, science\\\\n\\\\nAsians\\\\xa0 = civilization, art, literature, science \\\\n\\\\nNegroids = primitive tribal culture, still living in mud huts, no written language\"',\n",
       "        ...,\n",
       "        \"@evilfelicity No, but he did make a show of trying to answer a question about Norman Mailer. Didn't even know his fellow white dude trivia.\",\n",
       "        \"British lads Will and Steve are taking on the @mykitchenrules Instant Restaurant tonight and they're on now #MKR http://t.co/UA8kwVBEnn\",\n",
       "        'Nooooooo http://t.co/HVZT1QD5aI'], dtype=object),\n",
       " array([1., 0., 0., ..., 0., 0., 0.]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tup_MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'VisualizeNN'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-05c32783d156>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclassi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mVisualizeNN\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mVisNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneural_network\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'VisualizeNN'"
     ]
    }
   ],
   "source": [
    "classi = MLPClassifier\n",
    "\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "\n",
    "# this example won't converge because of CI's time constraints, so we catch the\n",
    "# warning and are ignore it here\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning,\n",
    "                            module=\"sklearn\")\n",
    "    mlp.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % mlp.score(X_test, y_test))\n",
    "\n",
    "fig, axes = plt.subplots(4, 4)\n",
    "# use global min / max to ensure all weights are shown on the same scale\n",
    "vmin, vmax = mlp.coefs_[0].min(), mlp.coefs_[0].max()\n",
    "for coef, ax in zip(mlp.coefs_[0].T, axes.ravel()):\n",
    "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5 * vmin,\n",
    "               vmax=.5 * vmax)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-82d6dcef3d9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtup_svr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_long\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSVR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPARAM_GRID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-6e646f3fe06b>\u001b[0m in \u001b[0;36mmake_pipeline\u001b[0;34m(df, clf, params, standardise)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_srch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dev\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Obtain predictions and save them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# obtain classification report of preds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mbest_est\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_srch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   1927\u001b[0m     \"\"\"\n\u001b[1;32m   1928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1929\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1931\u001b[0m     \u001b[0mlabels_given\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0m\u001b[1;32m     91\u001b[0m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "tup_svr = make_pipeline(df_long, SVR(), PARAM_GRID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tup_linear_svc = make_pipeline(df_long, LinearSVC(random_state=42), PARAM_GRID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tup_linear_svc_standardised = make_pipeline(df_long, LinearSVC(random_state=42), PARAM_GRID, standardise=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(tup_linear_svc[1]))\n",
    "print(type(tup_linear_svc[0]))\n",
    "print(tup_linear_svc[0].predict(['You are an absolute assohle. Hope your religion goes and dies!']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tup_svc_standardised = make_pipeline(df_long, SVC(random_state=42), PARAM_GRID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tup_svc = make_pipeline(df_long, SVC(random_state=42), PARAM_GRID, standardise=False)\n",
    "print(tup_svc[0])\n",
    "print(type(tup_svc[0][-1])) # tup_svc[0] is the best estimator, the [-1] accesses the kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################## LIME ###################\n",
    "Need explainer object, instance of test data = tuple_of_generators[1][whichever dataset performed best], instance of the classifier rf = pull from list tup_bots_forst[0][whichever dataset peformed best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tup_svc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a1d16bf5d668>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup_svc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2334\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup_svc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2334\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfitted_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtup_svc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfitted_clf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tup_svc' is not defined"
     ]
    }
   ],
   "source": [
    "print(tup_svc[2][2334])\n",
    "print(tup_svc[3][2334])\n",
    "\n",
    "fitted_clf = tup_svc[0][-1]\n",
    "print(fitted_clf)\n",
    "devset_x, devset_y = tup_svc[2], tup_svc[3] #accessing examples from the dataset for LIME\n",
    "\n",
    "\n",
    "def use_lime(clf, dev_x, dev_y):\n",
    "    from lime.lime_text import LimeTextExplainer\n",
    "    class_names = ['Hateful', 'Non-Hateful']\n",
    "    explainer = LimeTextExplainer(class_names=class_names)\n",
    "    idx = 2334\n",
    "    exp = explainer.explain_instance(dev_x[idx], clf.predict, num_features=6)\n",
    "    tweet = dev_y[idx]\n",
    "    label = dev_y[idx]\n",
    "    print('probability = ', clf.predict(dev_x[idx]))\n",
    "    print('true class: %d ' % label)\n",
    "    print('tweet: %s' % tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_lime(fitted_clf, devset_x, devset_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Neural Network Classifier \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tup_bots_MLP = bots(tuple_of_generators[0], tuple_of_generators[1], MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(10,30,10,5), random_state=42, batch_size=128, max_iter=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Support Vector Classifier without Gridsearch\n",
    "\n",
    "\n",
    "#tup_bots_svc = bots(tuple_of_generators[0], tuple_of_generators[1], SVC())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#RandomForestClassifier without Gridsearch\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "tup_bots_forest = bots(tuple_of_generators[0], tuple_of_generators[1], RandomForestClassifier(n_jobs=-1, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf = best_clf(tup_bots_forest, 1) #best performing classifier\n",
    "print(best_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tup_bots_forest[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hi(r):\n",
    "    lst = [1, 2, 3, 4, 5, 6, 7]\n",
    "    print(lst[0])\n",
    "    new_lst = []\n",
    "    for i in range(0, r):\n",
    "        clf = lst.pop()\n",
    "        new_lst.append(clf)\n",
    "    return new_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hi(3))\n",
    "print(hi(0))\n",
    "print(hi(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#classification report\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,recall_score,f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def report(prediction, real_labels, data):\n",
    "    print(\"classification report as follows: \")\n",
    "    print(f'   Accuracy: {accuracy_score(prediction, real_labels)}')\n",
    "    print(f'   Precision: {precision_score(prediction, real_labels,average=\"macro\")}')\n",
    "    print(f'   recall: {recall_score(prediction, real_labels,average=\"macro\")}')\n",
    "    print(f'   F1 measure: {f1_score(prediction, real_labels,average=\"macro\")}')\n",
    "    print('Show 5 example of correctly classified datapoint: ')\n",
    "    if data[prediction==real_labels].shape[0] > 5:\n",
    "        display(data[prediction==real_labels].iloc[:5,:])\n",
    "    else:\n",
    "        display(data[prediction==real_labels])\n",
    "    print('Show 5 example of wrongly classified datapoint: ')\n",
    "    if data[prediction!=real_labels].shape[0] > 5:\n",
    "        display(data[prediction!=real_labels].iloc[:5,:])\n",
    "    else:\n",
    "        display(data[prediction!=real_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_x = vectorizer.transform(validation_set['Tokens'])\n",
    "val_x = transformer.transform(val_x)\n",
    "\n",
    "predict = ntwk.predict(val_x)\n",
    "report(predict, validation_set['label'], validation_set[['Tokens','Label']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
