{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import nltk, spacy\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.preprocessing import STOPWORDS as SW\n",
    "from wordcloud import STOPWORDS\n",
    "from collections import Counter\n",
    "import enum\n",
    "\n",
    "pd.options.display.float_format = '{:,.0f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "in_file = Path.cwd().parents[0] / 'Processed_datasets' / 'tweets_long_no_duplicates.csv'\n",
    "my_stopwords = stopwords.words('english')\n",
    "stopwords = set(STOPWORDS).union(my_stopwords) #preparing stopwards list\n",
    "custom_stopwords = ['hi', '\\n', '\\n\\n', '&amp;', ' ', '.', '-',\n",
    "                    'got', \"it's\", 'it’s', \"i'm\", 'i’m', 'im', 'want', 'like', '$', '@']\n",
    "nlp = spacy.load('en_core_web_sm', exclude=['ner', 'tok2vec', 'tagger', 'paerser', 'senter', 'lemmatizer', 'attribute_ruler']) # using only for stopwords\n",
    "STOP_WORDS = nlp.Defaults.stop_words.union(custom_stopwords)\n",
    "ALL_STOP_WORDS = STOP_WORDS.union(SW).union(stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = pd.read_csv(in_file, engine='python', usecols=['Tokens', 'Label']) #not using unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Tokens  Label\n",
      "0   muslim mob violence against hindus in banglade...      1\n",
      "1   islamophobia is like the idea of naziphobia is...      1\n",
      "2   finally all caught up and that sudden death co...      0\n",
      "3   please please start using is your discernment ...      0\n",
      "4   as soon as isis chased all the minorities out ...      0\n",
      "5   islam invaded and conquered of christiandom be...      1\n",
      "6   do you approve of your pedophile prophet rapin...      1\n",
      "7   problem with vile muslims is that they try to ...      1\n",
      "8            tend to talk about it much personal info      0\n",
      "9   cool next time when a woman talks to him about...      0\n",
      "10  our judges are about to turn the heat up in th...      0\n",
      "11  lol this you walk by putting one foot in front...      0\n",
      "12  said wanted sorbet now and they should tell us...      0\n",
      "13  this fucking potato is blowing my mind duck fa...      0\n",
      "14  omg this churner feels like razor blades on my...      0\n",
      "0    15214\n",
      "1     7500\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_long.head(15))\n",
    "df_long.count()\n",
    "print(df_long['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up baseline pipeline\n",
    "from nltk.tokenize import TweetTokenizer #I chose to tokenize with this, as it gets rid of @ handlers\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "#tknzr = TweetTokenizer(preserve_case=True, reduce_len=True)\n",
    "#vect = TfidfVectorizer(max_df=.9, min_df=25,\n",
    "#                       strip_accents='unicode', tokenizer=tknzr.tokenize, stop_words=ALL_STOP_WORDS)\n",
    "#sampler_train = RandomUnderSampler(random_state=42) #undersampling to rebalance dataset\n",
    "#sampler_dev = RandomUnderSampler(random_state=50) #different samplers for each dataset\n",
    "#xs, ys = vect.fit_transform(df_long['Tokens'].values), df_long['Label'].values\n",
    "#x_train, x_dev, y_train, y_dev = train_test_split(\n",
    "#    xs, ys, train_size=.85, random_state=42, stratify=ys)\n",
    "#x_train, y_train = sampler_train.fit_resample(x_train, y_train) \n",
    "#x_dev, y_dev = sampler_dev.fit_resample(x_train, y_train)\n",
    "#print(Counter(y_train), Counter(y_dev)) #checking how many labels of each class are in each set \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_dataset(d_frame, stopwords=True, tfidf=True, train=True): #function for creating dataset, paramaters for flexibility\n",
    "    \"\"\"Helper function for construction the classes later on - accepts a dataframe \n",
    "    as an input and 3 boolean and ouputs a tuple of either the test or dev set\n",
    "    The code block essentially repeates itself with minor argument tweaks\"\"\"\n",
    "    df = d_frame\n",
    "    if stopwords and tfidf and train:                                                       #all included\n",
    "        tknzr = TweetTokenizer(preserve_case=True, reduce_len=True)\n",
    "        vect = TfidfVectorizer(max_df=.9, min_df=25, strip_accents='unicode',\n",
    "                               tokenizer=tknzr.tokenize, stop_words=ALL_STOP_WORDS)\n",
    "        sampler_train = RandomUnderSampler(random_state=42)                                  #undersampling  object to rebalance dataset\n",
    "        xs, ys = vect.fit_transform(df['Tokens'].values), df['Label'].values\n",
    "        x_train, x_dev, y_train, y_dev = train_test_split(xs, ys, train_size=.85, #creating train and dev sets with stratified sampling\n",
    "                                                          random_state=42, stratify=ys) \n",
    "        x_train, y_train = sampler_train.fit_resample(x_train, y_train) \n",
    "        return x_train, y_train\n",
    "    elif not stopwords and tfidf and train: #no stopwords\n",
    "        tknzr = TweetTokenizer(preserve_case=True, reduce_len=True)\n",
    "        vect = TfidfVectorizer(max_df=.9, min_df=25,strip_accents='unicode',\n",
    "                               tokenizer=tknzr.tokenize)\n",
    "        sampler_train = RandomUnderSampler(random_state=42) \n",
    "        xs, ys = vect.fit_transform(df['Tokens'].values), df['Label'].values\n",
    "        x_train, x_dev, y_train, y_dev = train_test_split(xs, ys, train_size=.85,\n",
    "                                                          random_state=42, stratify=ys)\n",
    "        x_train, y_train = sampler_train.fit_resample(x_train, y_train) \n",
    "        return x_train, y_train    \n",
    "    elif stopwords and not tfidf and train: #no tfidf\n",
    "        tknzr = TweetTokenizer(preserve_case=True, reduce_len=True)\n",
    "        vect = CountVectorizer(max_df=.9, min_df=25,\n",
    "                       strip_accents='unicode', tokenizer=tknzr.tokenize, stop_words=ALL_STOP_WORDS)\n",
    "        sampler_train = RandomUnderSampler(random_state=42) \n",
    "        xs, ys = vect.fit_transform(df['Tokens'].values), df['Label'].values\n",
    "        x_train, x_dev, y_train, y_dev = train_test_split(xs, ys, train_size=.85,\n",
    "                                                          random_state=42, stratify=ys)\n",
    "        x_train, y_train = sampler_train.fit_resample(x_train, y_train) \n",
    "        return x_train, y_train\n",
    "    elif not stopwords and not tfidf and train:\n",
    "        tknzr = TweetTokenizer(preserve_case=True, reduce_len=True)\n",
    "        vect = CountVectorizer(max_df=.9, min_df=25,\n",
    "                       strip_accents='unicode', tokenizer=tknzr.tokenize)\n",
    "        sampler_train = RandomUnderSampler(random_state=42) \n",
    "        xs, ys = vect.fit_transform(df['Tokens'].values), df['Label'].values\n",
    "        x_train, x_dev, y_train, y_dev = train_test_split(xs, ys, train_size=.85,\n",
    "                                                          random_state=42, stratify=ys)\n",
    "        x_train, y_train = sampler_train.fit_resample(x_train, y_train) \n",
    "        return x_train, y_train\n",
    "    elif stopwords and tfidf and not train:\n",
    "        tknzr = TweetTokenizer(preserve_case=True, reduce_len=True)\n",
    "        vect = TfidfVectorizer(max_df=.9, min_df=25,\n",
    "                       strip_accents='unicode', tokenizer=tknzr.tokenize, stop_words=ALL_STOP_WORDS)\n",
    "        sampler_dev = RandomUnderSampler(random_state=50) \n",
    "        xs, ys = vect.fit_transform(df['Tokens'].values), df['Label'].values\n",
    "        x_train, x_dev, y_train, y_dev = train_test_split(xs, ys, train_size=.85,\n",
    "                                                          random_state=42, stratify=ys)\n",
    "        x_dev, y_dev = sampler_dev.fit_resample(x_train, y_train)\n",
    "        return x_dev, y_dev\n",
    "    elif not stopwords and tfidf and not train:\n",
    "        tknzr = TweetTokenizer(preserve_case=True, reduce_len=True)\n",
    "        vect = TfidfVectorizer(max_df=.9, min_df=25,strip_accents='unicode',\n",
    "                               tokenizer=tknzr.tokenize)\n",
    "        sampler_dev = RandomUnderSampler(random_state=50) \n",
    "        xs, ys = vect.fit_transform(df['Tokens'].values), df['Label'].values\n",
    "        x_train, x_dev, y_train, y_dev = train_test_split(xs, ys, train_size=.85,\n",
    "                                                          random_state=42, stratify=ys)\n",
    "        x_dev, y_dev = sampler_dev.fit_resample(x_train, y_train)\n",
    "        return x_dev, y_dev\n",
    "    elif stopwords and not tfidf and not train:\n",
    "        tknzr = TweetTokenizer(preserve_case=True, reduce_len=True)\n",
    "        vect = CountVectorizer(max_df=.9, min_df=25,strip_accents='unicode',\n",
    "                               tokenizer=tknzr.tokenize, stopwords=ALL_STOP_WORDS)\n",
    "        sampler_dev = RandomUnderSampler(random_state=50) \n",
    "        xs, ys = vect.fit_transform(df['Tokens'].values), df['Label'].values\n",
    "        x_train, x_dev, y_train, y_dev = train_test_split(xs, ys, train_size=.85,\n",
    "                                                          random_state=42, stratify=ys)\n",
    "        x_dev, y_dev = sampler_dev.fit_resample(x_train, y_train)\n",
    "        return x_dev, y_dev\n",
    "    else:\n",
    "        tknzr = TweetTokenizer(preserve_case=True, reduce_len=True)\n",
    "        vect = CountVectorizer(max_df=.9, min_df=25,strip_accents='unicode',\n",
    "                               tokenizer=tknzr.tokenize)\n",
    "        sampler_dev = RandomUnderSampler(random_state=50) \n",
    "        xs, ys = vect.fit_transform(df_long['Tokens'].values), df_long['Label'].values\n",
    "        x_train, x_dev, y_train, y_dev = train_test_split(xs, ys, train_size=.85,\n",
    "                                                          random_state=42, stratify=ys)\n",
    "        x_dev, y_dev = sampler_dev.fit_resample(x_train, y_train)\n",
    "        return x_dev, y_dev    \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amaan\\anaconda3\\envs\\t_mining\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['&', \"'\", 'n', '‘', '’'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\amaan\\anaconda3\\envs\\t_mining\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['&', \"'\", 'n', '‘', '’'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'> (<12750x1454 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 63969 stored elements in Compressed Sparse Row format>, array([0, 0, 0, ..., 1, 1, 1], dtype=int64))\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "#all_param_train = Dataset(df_long)\n",
    "#no_param_train = Dataset(df_long, stopwords=False, tfidf=False, train=True)\n",
    "#train_set = all_param_train.make_dataset()\n",
    "#print(train_set[0:10])\n",
    "\n",
    "#print(type(no_param_train.make_dataset()))\n",
    "\n",
    "all_param_train = make_dataset(df_long)\n",
    "all_param_dev = make_dataset(df_long, train=False)\n",
    "no_param_train = make_dataset(df_long, stopwords=False, tfidf=False)\n",
    "no_param_dev = make_dataset(df_long, stopwords=False, tfidf=False, train=False)\n",
    "print(type(all_param_train), all_param_train)\n",
    "print(type(all_param_dev[0]))\n",
    "print(type(no_param_dev[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierBot():\n",
    "    \n",
    "    def __init__(self, trainset, devset, bot):\n",
    "        self.train = trainset\n",
    "        self.dev = devset\n",
    "        self.bot = bot\n",
    "        \n",
    "    def classify(self):\n",
    "        self.bot.fit(self.train[0], self.train[1])\n",
    "        #self.bot.predict(['islam invaded and conquered of christiandom before any christian crusades in response the writer is a liar'])\n",
    "        prediction = self.bot.predict(self.dev[0])\n",
    "        return classification_report(self.dev[1], prediction)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.84      0.79      6375\n",
      "           1       0.82      0.72      0.77      6375\n",
      "\n",
      "    accuracy                           0.78     12750\n",
      "   macro avg       0.79      0.78      0.78     12750\n",
      "weighted avg       0.79      0.78      0.78     12750\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.85      0.81      6375\n",
      "           1       0.83      0.75      0.79      6375\n",
      "\n",
      "    accuracy                           0.80     12750\n",
      "   macro avg       0.80      0.80      0.80     12750\n",
      "weighted avg       0.80      0.80      0.80     12750\n",
      "\n",
      "SGDClassifier()\n",
      "<__main__.ClassifierBot object at 0x000001C0200A9F88>\n"
     ]
    }
   ],
   "source": [
    "sgd_clf = SGDClassifier()\n",
    "bot1 = ClassifierBot(all_param_train, all_param_dev, sgd_clf)\n",
    "bot_no_params = ClassifierBot(no_param_train, no_param_dev, sgd_clf)\n",
    "print(bot1.classify())\n",
    "print(bot_no_params.classify())\n",
    "print(sgd_clf)\n",
    "print(bot1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification report\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,recall_score,f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def report(prediction, real_labels, data):\n",
    "    print(\"classification report as follows: \")\n",
    "    print(f'   Accuracy: {accuracy_score(prediction, real_labels)}')\n",
    "    print(f'   Precision: {precision_score(prediction, real_labels,average=\"macro\")}')\n",
    "    print(f'   recall: {recall_score(prediction, real_labels,average=\"macro\")}')\n",
    "    print(f'   F1 measure: {f1_score(prediction, real_labels,average=\"macro\")}')\n",
    "    print('Show 5 example of correctly classified datapoint: ')\n",
    "    if data[prediction==real_labels].shape[0] > 5:\n",
    "        display(data[prediction==real_labels].iloc[:5,:])\n",
    "    else:\n",
    "        display(data[prediction==real_labels])\n",
    "    print('Show 5 example of wrongly classified datapoint: ')\n",
    "    if data[prediction!=real_labels].shape[0] > 5:\n",
    "        display(data[prediction!=real_labels].iloc[:5,:])\n",
    "    else:\n",
    "        display(data[prediction!=real_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierBot():\n",
    "    \n",
    "    def __init__(self, trainset, devset, bot):\n",
    "        self.train = trainset\n",
    "        self.dev = devset\n",
    "        self.bot = bot\n",
    "        \n",
    "    def classify(self):\n",
    "        self.bot.fit(self.train[0], self.train[1])\n",
    "        #self.bot.predict(['islam invaded and conquered of christiandom before any christian crusades in response the writer is a liar'])\n",
    "        prediction = self.bot.predict(self.dev[0])\n",
    "        return report(prediction, self.dev[1]. self.dev)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-4ef062134549>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbot1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mClassifierBot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_param_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_param_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd_clf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mbot_no_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mClassifierBot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mno_param_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_param_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd_clf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbot1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbot_no_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msgd_clf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-51-4ec9f1753b90>\u001b[0m in \u001b[0;36mclassify\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m#self.bot.predict(['islam invaded and conquered of christiandom before any christian crusades in response the writer is a liar'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdev\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mreport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdev\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'self'"
     ]
    }
   ],
   "source": [
    "sgd_clf = SGDClassifier()\n",
    "bot1 = ClassifierBot(all_param_train, all_param_dev, sgd_clf)\n",
    "bot_no_params = ClassifierBot(no_param_train, no_param_dev, sgd_clf)\n",
    "print(bot1.classify())\n",
    "print(bot_no_params.classify())\n",
    "print(sgd_clf)\n",
    "print(bot1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='newton-cg', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Baseline: logistic regression!\n",
    "#Same as Question 4\n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "vectorizer=CountVectorizer(stop_words='english',max_features=100)\n",
    "transformer=TfidfTransformer()\n",
    "\n",
    "vectorizer.fit(train_set['Tokens'].to_list())\n",
    "train_x = vectorizer.transform(train_set['Tokens'].to_list())\n",
    "\n",
    "transformer.fit(train_x)\n",
    "train_x = transformer.transform(train_x)\n",
    "\n",
    "train_y = train_set['label']\n",
    "lr = LogisticRegression(multi_class=\"multinomial\", solver=\"newton-cg\")\n",
    "lr.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report as follows: \n",
      "   Accuracy: 0.7987216443163304\n",
      "   Precision: 0.6614977192076772\n",
      "   recall: 0.7667356382049939\n",
      "   F1 measure: 0.6835940478433892\n",
      "Show 5 example of correctly classified datapoint: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23121</th>\n",
       "      <td>rt strategic vote kat food truly awful #mkr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35928</th>\n",
       "      <td>pancakes proof deity love us #mkr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45316</th>\n",
       "      <td>sick see fuck asshole bitch make chain latters...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42230</th>\n",
       "      <td>i'm try get insight trans issue definitely gro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40128</th>\n",
       "      <td>make</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tokens  Label\n",
       "23121        rt strategic vote kat food truly awful #mkr      0\n",
       "35928                  pancakes proof deity love us #mkr      0\n",
       "45316  sick see fuck asshole bitch make chain latters...      1\n",
       "42230  i'm try get insight trans issue definitely gro...      0\n",
       "40128                                               make      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show 5 example of wrongly classified datapoint: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48658</th>\n",
       "      <td>lol ralph guy still moi era</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28396</th>\n",
       "      <td>single men cannot adopt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25704</th>\n",
       "      <td>shame katie nikki kat go #mkr</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44924</th>\n",
       "      <td>muslim brotherhood usa hundred years liken say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7346</th>\n",
       "      <td>rt #mosul christian pastor #paul_jacob sentenc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tokens  Label\n",
       "48658                        lol ralph guy still moi era      1\n",
       "28396                            single men cannot adopt      1\n",
       "25704                      shame katie nikki kat go #mkr      1\n",
       "44924  muslim brotherhood usa hundred years liken say...      0\n",
       "7346   rt #mosul christian pastor #paul_jacob sentenc...      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_x = vectorizer.transform(validation_set['Tokens'])\n",
    "val_x = transformer.transform(val_x)\n",
    "\n",
    "predict = lr.predict(val_x)\n",
    "report(predict, validation_set['label'], validation_set[['Tokens','Label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############END OF TF-IDF##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report as follows: \n",
      "   Accuracy: 0.7894472991602958\n",
      "   Precision: 0.6323198258641166\n",
      "   recall: 0.7665798483965244\n",
      "   F1 measure: 0.6501280393689739\n",
      "Show 5 example of correctly classified datapoint: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23121</th>\n",
       "      <td>rt strategic vote kat food truly awful #mkr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35928</th>\n",
       "      <td>pancakes proof deity love us #mkr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42230</th>\n",
       "      <td>i'm try get insight trans issue definitely gro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40128</th>\n",
       "      <td>make</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3114</th>\n",
       "      <td>wut</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tokens  Label\n",
       "23121        rt strategic vote kat food truly awful #mkr      0\n",
       "35928                  pancakes proof deity love us #mkr      0\n",
       "42230  i'm try get insight trans issue definitely gro...      0\n",
       "40128                                               make      0\n",
       "3114                                                 wut      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show 5 example of wrongly classified datapoint: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45316</th>\n",
       "      <td>sick see fuck asshole bitch make chain latters...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48658</th>\n",
       "      <td>lol ralph guy still moi era</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28396</th>\n",
       "      <td>single men cannot adopt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25704</th>\n",
       "      <td>shame katie nikki kat go #mkr</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44924</th>\n",
       "      <td>muslim brotherhood usa hundred years liken say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tokens  Label\n",
       "45316  sick see fuck asshole bitch make chain latters...      1\n",
       "48658                        lol ralph guy still moi era      1\n",
       "28396                            single men cannot adopt      1\n",
       "25704                      shame katie nikki kat go #mkr      1\n",
       "44924  muslim brotherhood usa hundred years liken say...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_x = vectorizer.transform(validation_set['Tokens'])\n",
    "val_x = transformer.transform(val_x)\n",
    "\n",
    "predict = clf.predict(val_x)\n",
    "report(predict, validation_set['label'], validation_set[['Tokens','Label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################END OF MULTINOMINAL NAIVE BAYES######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size=128, beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(10, 30, 10, 5), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=400,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=1, shuffle=True, solver='adam',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Neural Network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "ntwk = MLPClassifier(solver='adam', alpha=1e-5,hidden_layer_sizes=(10,30,10,5), random_state=1,batch_size=128,max_iter=400)\n",
    "ntwk.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report as follows: \n",
      "   Accuracy: 0.8168943476626144\n",
      "   Precision: 0.7084592624109877\n",
      "   recall: 0.7811589138333501\n",
      "   F1 measure: 0.7309291098045785\n",
      "Show 5 example of correctly classified datapoint: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23121</th>\n",
       "      <td>rt strategic vote kat food truly awful #mkr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35928</th>\n",
       "      <td>pancakes proof deity love us #mkr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45316</th>\n",
       "      <td>sick see fuck asshole bitch make chain latters...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42230</th>\n",
       "      <td>i'm try get insight trans issue definitely gro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40128</th>\n",
       "      <td>make</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tokens  Label\n",
       "23121        rt strategic vote kat food truly awful #mkr      0\n",
       "35928                  pancakes proof deity love us #mkr      0\n",
       "45316  sick see fuck asshole bitch make chain latters...      1\n",
       "42230  i'm try get insight trans issue definitely gro...      0\n",
       "40128                                               make      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show 5 example of wrongly classified datapoint: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48658</th>\n",
       "      <td>lol ralph guy still moi era</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30437</th>\n",
       "      <td>kill muslims oppose kill ezidis christians non...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28396</th>\n",
       "      <td>single men cannot adopt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44924</th>\n",
       "      <td>muslim brotherhood usa hundred years liken say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7346</th>\n",
       "      <td>rt #mosul christian pastor #paul_jacob sentenc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tokens  Label\n",
       "48658                        lol ralph guy still moi era      1\n",
       "30437  kill muslims oppose kill ezidis christians non...      0\n",
       "28396                            single men cannot adopt      1\n",
       "44924  muslim brotherhood usa hundred years liken say...      0\n",
       "7346   rt #mosul christian pastor #paul_jacob sentenc...      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_x = vectorizer.transform(validation_set['Tokens'])\n",
    "val_x = transformer.transform(val_x)\n",
    "\n",
    "predict = ntwk.predict(val_x)\n",
    "report(predict, validation_set['label'], validation_set[['Tokens','Label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
