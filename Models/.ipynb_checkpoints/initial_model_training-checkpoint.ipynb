{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  #Imports for converting dataframe to train_dev splits\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import nltk, spacy\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import STOPWORDS\n",
    "from collections import Counter\n",
    "from gensim.parsing.preprocessing import STOPWORDS as SW\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "pd.options.display.float_format = '{:,.0f}'.format\n",
    "in_file = Path.cwd().parents[0] / 'Processed_datasets' / 'final_db.csv'\n",
    "my_stopwords = stopwords.words('english')\n",
    "stopwords = set(STOPWORDS).union(my_stopwords) #preparing stopwards list\n",
    "custom_stopwords = ['hi', '\\n', '\\n\\n', '&amp;', ' ', '.', '-',\n",
    "                    'got', \"it's\", 'it’s', \"i'm\", 'i’m', 'im', 'want', 'like', '$', '@']\n",
    "nlp = spacy.load('en_core_web_sm', exclude=['ner', 'tok2vec', 'tagger', 'paerser', 'senter', 'lemmatizer', 'attribute_ruler']) # using only for stopwords\n",
    "STOP_WORDS = nlp.Defaults.stop_words.union(custom_stopwords)\n",
    "ALL_STOP_WORDS = STOP_WORDS.union(SW).union(stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = pd.read_csv(in_file, engine='python', usecols=['Text', 'oh_label'], encoding='utf-8') #not using unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Text  oh_label\n",
      "0   @AAlwuhaib1977 Muslim mob violence against Hin...         1\n",
      "1              @Te4m_NiGhtM4Re http://t.co/5Ih7MkDbQG         0\n",
      "2   @jncatron @isra_jourisra @AMPalestine Islamoph...         1\n",
      "3   Finally I'm all caught up, and that sudden dea...         0\n",
      "4              @carolinesinders @herecomesfran *hugs*         0\n",
      "5   Please, PLEASE start using \"is your discernmen...         0\n",
      "6   @aymannathem As soon as ISIS chased all the mi...         0\n",
      "7   @Ali_Gharib @MaxBlumenthal Glad you like it. h...         0\n",
      "8   @HuffPostRelig Islam invaded and conquered 2/3...         1\n",
      "9   @semzyxx Do you approve of your pedophile prop...         1\n",
      "10  @watan71969 @geeky_zekey Problem with vile Mus...         1\n",
      "11  @Skawtnyc @athenahollow @twoscooters i don't t...         0\n",
      "12  @dylanw that's cool. next time when a woman ta...         0\n",
      "13  RT @hadi_elis: Erdogan's Egyptian Nightmare \\r...         0\n",
      "14  RT @mykitchenrules: Our judges are about to tu...         0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text        25596\n",
       "oh_label    25596\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_long.head(15))\n",
    "df_long.count()\n",
    "#print(df_long['Text'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "<class 'numpy.float64'>\n",
      "0.0\n",
      "<class 'numpy.float64'>\n",
      "1.0\n",
      "<class 'numpy.float64'>\n",
      "0.0\n",
      "<class 'numpy.float64'>\n",
      "0.0\n",
      "<class 'numpy.float64'>\n",
      "0.0\n",
      "<class 'numpy.float64'>\n",
      "0.0\n",
      "<class 'numpy.float64'>\n",
      "0.0\n",
      "<class 'numpy.float64'>\n",
      "1.0\n",
      "<class 'numpy.float64'>\n",
      "1.0\n",
      "<class 'numpy.float64'>\n",
      "@AAlwuhaib1977 Muslim mob violence against Hindus in Bangladesh continues in 2014. #Islam http://t.co/C1JBWJwuRc\n",
      "<class 'str'>\n",
      "@Te4m_NiGhtM4Re http://t.co/5Ih7MkDbQG\n",
      "<class 'str'>\n",
      "@jncatron @isra_jourisra @AMPalestine Islamophobia is like the idea of Naziphobia. Islam is a religion of hate and it must be outlawed.\n",
      "<class 'str'>\n",
      "Finally I'm all caught up, and that sudden death cook off looks like it's gonna be intense #MKR\n",
      "<class 'str'>\n",
      "@carolinesinders @herecomesfran *hugs*\n",
      "<class 'str'>\n",
      "Please, PLEASE start using \"is your discernment blunted by steroids\" to mean \"are you on DRUGS?\" from now on. DEAD\n",
      "<class 'str'>\n",
      "@aymannathem As soon as ISIS chased all the minorities out of Mosul, the Sunni Arabs were happy to steal their property.  So fuck them.\n",
      "<class 'str'>\n",
      "@Ali_Gharib @MaxBlumenthal Glad you like it. http://t.co/3ME3Nrk8xZ\n",
      "<class 'str'>\n",
      "@HuffPostRelig Islam invaded and conquered 2/3 of Christiandom before any Christian crusades in response. The writer is a liar.\n",
      "<class 'str'>\n",
      "@semzyxx Do you approve of your pedophile prophet raping a 9 year old girl, like it says in 7 hadith?\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "labels = df_long.iloc[:, 1].values\n",
    "for l in labels[:10]:\n",
    "    print(l)\n",
    "    print(type(l))\n",
    "toks = df_long.iloc[:, 0].values\n",
    "for l in toks[:10]:\n",
    "    print(l)\n",
    "    print(type(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#setting up baseline pipeline\n",
    "from nltk.tokenize import TweetTokenizer #I chose to tokenize with this, as it gets rid of @ handlers\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #easy idf and stopword removal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler #moderately imbalanced dataset so randomundersampling - approx 2:1 ratio\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC, SVC, SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "PARAM_GRID = [{\n",
    "        'vect__stop_words': [None, ALL_STOP_WORDS],\n",
    "        'clf__kernel' : ['sigmoid', 'rbf'], # for SVC\n",
    "        #'clf__loss' : ['hinge', 'squared_hinge'],\n",
    "        #'clf__activation' : ['tanh', 'relu', 'logistic'], #for MLP\n",
    "        #'clf__hidden_layer_sizes' : [(100,), (10, 20, 30), (15, 25, 10), (40, 30, 25)],\n",
    "        'vect__use_idf' : [True, False]\n",
    "    }]\n",
    "\n",
    "\n",
    "def pipe(df, clf, params, standardise=True):\n",
    "    if standardise == True:\n",
    "        xs, ys = df['Text'].values, df['oh_label'].values\n",
    "        x_train, x_dev, y_train, y_dev = train_test_split(\n",
    "            xs, ys, train_size=.85, random_state=42, stratify=ys)\n",
    "        tknzr = TweetTokenizer(preserve_case=True, reduce_len=True, strip_handles=True)\n",
    "        vect = TfidfVectorizer(\n",
    "            max_df=.9, min_df=25, strip_accents='unicode', tokenizer=tknzr.tokenize)\n",
    "        scaler = StandardScaler(with_mean=False)\n",
    "        sampler = RandomUnderSampler(random_state=42)\n",
    "        pipeline = Pipeline([('vect', vect), ('scale', scaler),\n",
    "                             ('sampler', sampler), ('clf', clf)])\n",
    "        grid_srch = GridSearchCV(\n",
    "            estimator=pipeline, param_grid=params, refit=True, n_jobs=-1)\n",
    "        grid_srch.fit(x_train, y_train)  # fit the grid_search object\n",
    "        prediction = grid_srch.predict(x_dev)  # Obtain predictions and save them\n",
    "        # obtain classification report of preds\n",
    "        report = classification_report(y_dev, prediction, output_dict=True)\n",
    "        best_est = grid_srch.best_estimator_\n",
    "        print(report)\n",
    "        return best_est, prediction, x_dev, y_dev\n",
    "    xs, ys = df['Text'].values, df['oh_label'].values\n",
    "    x_train, x_dev, y_train, y_dev = train_test_split(\n",
    "         xs, ys, train_size=.85, random_state=42, stratify=ys)\n",
    "    tknzr = TweetTokenizer(preserve_case=True, reduce_len=True, strip_handles=True)\n",
    "    vect = TfidfVectorizer(max_df=.9, min_df=25, strip_accents='unicode', tokenizer=tknzr.tokenize)\n",
    "    sampler = RandomUnderSampler(random_state=42)\n",
    "    pipeline = make_pipeline(vect, sampler, clf)\n",
    "    grid_srch = GridSearchCV(\n",
    "        estimator=pipeline, param_grid=params, refit=True, n_jobs=-1)\n",
    "    fitted = grid_srch.fit(x_train, y_train)  # fit the grid_search object\n",
    "    prediction = grid_srch.predict(x_dev)  # Obtain predictions and save them\n",
    "    # obtain classification report of preds\n",
    "    report = classification_report(y_dev, prediction, output_dict=True)\n",
    "    best_est = fitted.best_estimator_\n",
    "    print(report)\n",
    "    return best_est, prediction, x_dev, y_dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "tup_MLP = make_pipeline(df_long, MLPClassifier(random_state=42), PARAM_GRID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-82d6dcef3d9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtup_svr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_long\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSVR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPARAM_GRID\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-60-983fb6490790>\u001b[0m in \u001b[0;36mmake_pipeline\u001b[1;34m(df, clf, params, standardise)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_srch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_dev\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Obtain predictions and save them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# obtain classification report of preds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mbest_est\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_srch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\t_mining\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\t_mining\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   1964\u001b[0m     \"\"\"\n\u001b[0;32m   1965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1966\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1967\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1968\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\t_mining\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[1;32m---> 93\u001b[1;33m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "tup_svr = make_pipeline(df_long, SVR(), PARAM_GRID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amaan\\anaconda3\\envs\\t_mining\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['&', \"'\", 'n', '‘', '’'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 0.8294172932330827, 'recall': 0.7731055628558914, 'f1-score': 0.8002720471548402, 'support': 2283}, '1': {'precision': 0.5953125, 'recall': 0.6773333333333333, 'f1-score': 0.6336798336798337, 'support': 1125}, 'accuracy': 0.7414906103286385, 'macro avg': {'precision': 0.7123648966165413, 'recall': 0.7252194480946124, 'f1-score': 0.716975940417337, 'support': 3408}, 'weighted avg': {'precision': 0.7521379820865985, 'recall': 0.7414906103286385, 'f1-score': 0.7452790189390589, 'support': 3408}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amaan\\anaconda3\\envs\\t_mining\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "tup_linear_svc = make_pipeline(df_long, LinearSVC(random_state=42), PARAM_GRID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amaan\\anaconda3\\envs\\t_mining\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['&', \"'\", 'n', '‘', '’'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 0.8294172932330827, 'recall': 0.7731055628558914, 'f1-score': 0.8002720471548402, 'support': 2283}, '1': {'precision': 0.5953125, 'recall': 0.6773333333333333, 'f1-score': 0.6336798336798337, 'support': 1125}, 'accuracy': 0.7414906103286385, 'macro avg': {'precision': 0.7123648966165413, 'recall': 0.7252194480946124, 'f1-score': 0.716975940417337, 'support': 3408}, 'weighted avg': {'precision': 0.7521379820865985, 'recall': 0.7414906103286385, 'f1-score': 0.7452790189390589, 'support': 3408}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amaan\\anaconda3\\envs\\t_mining\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "tup_linear_svc_standardised = make_pipeline(df_long, LinearSVC(random_state=42), PARAM_GRID, standardise=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3408\n",
      "<class 'imblearn.pipeline.Pipeline'>\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(len(tup_linear_svc[1]))\n",
    "print(type(tup_linear_svc[0]))\n",
    "print(tup_linear_svc[0].predict(['You are an absolute assohle. Hope your religion goes and dies!']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0.0': {'precision': 0.860858794384806, 'recall': 0.7954979015642885, 'f1-score': 0.826888756692445, 'support': 2621}, '1.0': {'precision': 0.6220028208744711, 'recall': 0.7235438884331419, 'f1-score': 0.6689419795221844, 'support': 1219}, 'accuracy': 0.77265625, 'macro avg': {'precision': 0.7414308076296385, 'recall': 0.7595208949987151, 'f1-score': 0.7479153681073147, 'support': 3840}, 'weighted avg': {'precision': 0.7850344632105617, 'recall': 0.77265625, 'f1-score': 0.7767488813355315, 'support': 3840}}\n"
     ]
    }
   ],
   "source": [
    "tup_svc = pipe(df_long, SVC(random_state=42, probability=True), PARAM_GRID, standardise=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('vect',\n",
      "                 TfidfVectorizer(max_df=0.9, min_df=25, strip_accents='unicode',\n",
      "                                 tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x000001F8C8B6F888>>,\n",
      "                                 use_idf=False)),\n",
      "                ('scale', StandardScaler(with_mean=False)),\n",
      "                ('sampler', RandomUnderSampler(random_state=42)),\n",
      "                ('clf',\n",
      "                 SVC(kernel='sigmoid', probability=True, random_state=42))]) SVC(kernel='sigmoid', probability=True, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "best_est = tup_svc[0]\n",
    "best_clf = best_est[-1]\n",
    "devset_x, devset_y = tup_svc[2], tup_svc[3] #accessing examples from the dataset for LIME\n",
    "print(best_est, best_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################## LIME ###################\n",
    "Need explainer object, instance of test data = tuple_of_generators[1][whichever dataset performed best], instance of the classifier rf = pull from list tup_bots_forst[0][whichever dataset peformed best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3840\n",
      "3840\n"
     ]
    }
   ],
   "source": [
    "print(len(devset_y))\n",
    "print(len(devset_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@UmarMal compared to the sewer and violence and poverty of Muslim Pakistan, Americans live in opulence.\n"
     ]
    }
   ],
   "source": [
    "#print(tup_svc[2][2334])\n",
    "#print(tup_svc[3][2334])\n",
    "\n",
    "\n",
    "print(devset_x[10])\n",
    "\n",
    "def use_lime(clf, dev_x, dev_y):\n",
    "    idx = 333\n",
    "    class_names = [0, 1]\n",
    "    tweet = dev_x[idx]\n",
    "    label = dev_y[idx]\n",
    "    print(tweet)\n",
    "    print(type(tweet), label)\n",
    "    explainer = LimeTextExplainer(class_names=class_names)\n",
    "    exp = explainer.explain_instance(dev_x[idx], best_clf.predict_proba, num_features=6)\n",
    "    print('probability = ', clf.predict_proba(dev_x[idx]))\n",
    "    print('true class: %d ' % label)\n",
    "    print('tweet: %s' % tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@semzyxx @NAInfidels @owais00 As you can see for yourself, pedophelia is illegal in Israel, but is legal in Muslim countries. Outlaw Islam!\n",
      "<class 'str'> 1.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '@semzyxx @NAInfidels @owais00 As you can see for yourself, pedophelia is illegal in Israel, but is legal in Muslim countries. Outlaw Islam!'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-426f21959cac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0muse_lime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfitted_clf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevset_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevset_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-77-15217613301b>\u001b[0m in \u001b[0;36muse_lime\u001b[1;34m(clf, dev_x, dev_y)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mexplainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLimeTextExplainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mexp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplain_instance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'probability = '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'true class: %d '\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\t_mining\\lib\\site-packages\\lime\\lime_text.py\u001b[0m in \u001b[0;36mexplain_instance\u001b[1;34m(self, text_instance, classifier_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[0;32m    413\u001b[0m         data, yss, distances = self.__data_labels_distances(\n\u001b[0;32m    414\u001b[0m             \u001b[0mindexed_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m             distance_metric=distance_metric)\n\u001b[0m\u001b[0;32m    416\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_names\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\t_mining\\lib\\site-packages\\lime\\lime_text.py\u001b[0m in \u001b[0;36m__data_labels_distances\u001b[1;34m(self, indexed_string, classifier_fn, num_samples, distance_metric)\u001b[0m\n\u001b[0;32m    480\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minactive\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m             \u001b[0minverse_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexed_string\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_removing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minactive\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minverse_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistance_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\t_mining\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_predict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    671\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobA_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobB_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m             raise NotFittedError(\"predict_proba is not available when fitted \"\n",
      "\u001b[1;32m~\\anaconda3\\envs\\t_mining\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m             X = check_array(X, accept_sparse='csr', dtype=np.float64,\n\u001b[1;32m--> 475\u001b[1;33m                             order=\"C\", accept_large_sparse=False)\n\u001b[0m\u001b[0;32m    476\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\t_mining\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\t_mining\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    614\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 616\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    617\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\anaconda3\\envs\\t_mining\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order, like)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '@semzyxx @NAInfidels @owais00 As you can see for yourself, pedophelia is illegal in Israel, but is legal in Muslim countries. Outlaw Islam!'"
     ]
    }
   ],
   "source": [
    "use_lime(fitted_clf, devset_x, devset_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Neural Network Classifier \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tup_bots_MLP = bots(tuple_of_generators[0], tuple_of_generators[1], MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(10,30,10,5), random_state=42, batch_size=128, max_iter=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 0.7183863460046548, 'recall': 0.8231111111111111, 'f1-score': 0.767191383595692, 'support': 1125}, '1': {'precision': 0.7929240374609782, 'recall': 0.6773333333333333, 'f1-score': 0.7305848513902207, 'support': 1125}, 'accuracy': 0.7502222222222222, 'macro avg': {'precision': 0.7556551917328165, 'recall': 0.7502222222222222, 'f1-score': 0.7488881174929563, 'support': 2250}, 'weighted avg': {'precision': 0.7556551917328165, 'recall': 0.7502222222222222, 'f1-score': 0.7488881174929563, 'support': 2250}}\n",
      "<class 'dict'>\n",
      "{'0': {'precision': 0.7123893805309734, 'recall': 0.8586666666666667, 'f1-score': 0.7787182587666264, 'support': 1125}, '1': {'precision': 0.8221476510067114, 'recall': 0.6533333333333333, 'f1-score': 0.7280832095096582, 'support': 1125}, 'accuracy': 0.756, 'macro avg': {'precision': 0.7672685157688424, 'recall': 0.756, 'f1-score': 0.7534007341381423, 'support': 2250}, 'weighted avg': {'precision': 0.7672685157688424, 'recall': 0.756, 'f1-score': 0.7534007341381422, 'support': 2250}}\n",
      "<class 'dict'>\n",
      "{'0': {'precision': 0.7389558232931727, 'recall': 0.8177777777777778, 'f1-score': 0.7763713080168776, 'support': 1125}, '1': {'precision': 0.7960199004975125, 'recall': 0.7111111111111111, 'f1-score': 0.7511737089201879, 'support': 1125}, 'accuracy': 0.7644444444444445, 'macro avg': {'precision': 0.7674878618953426, 'recall': 0.7644444444444445, 'f1-score': 0.7637725084685327, 'support': 2250}, 'weighted avg': {'precision': 0.7674878618953426, 'recall': 0.7644444444444445, 'f1-score': 0.7637725084685327, 'support': 2250}}\n",
      "<class 'dict'>\n",
      "{'0': {'precision': 0.7105064247921391, 'recall': 0.8355555555555556, 'f1-score': 0.7679738562091503, 'support': 1125}, '1': {'precision': 0.8004314994606256, 'recall': 0.6595555555555556, 'f1-score': 0.7231968810916178, 'support': 1125}, 'accuracy': 0.7475555555555555, 'macro avg': {'precision': 0.7554689621263824, 'recall': 0.7475555555555555, 'f1-score': 0.745585368650384, 'support': 2250}, 'weighted avg': {'precision': 0.7554689621263824, 'recall': 0.7475555555555555, 'f1-score': 0.745585368650384, 'support': 2250}}\n",
      "<class 'dict'>\n",
      "{'0': {'precision': 0.7109872611464968, 'recall': 0.7937777777777778, 'f1-score': 0.750104997900042, 'support': 1125}, '1': {'precision': 0.7665995975855131, 'recall': 0.6773333333333333, 'f1-score': 0.7192071731949032, 'support': 1125}, 'accuracy': 0.7355555555555555, 'macro avg': {'precision': 0.738793429366005, 'recall': 0.7355555555555555, 'f1-score': 0.7346560855474726, 'support': 2250}, 'weighted avg': {'precision': 0.7387934293660049, 'recall': 0.7355555555555555, 'f1-score': 0.7346560855474726, 'support': 2250}}\n",
      "<class 'dict'>\n",
      "{'0': {'precision': 0.702962962962963, 'recall': 0.8435555555555555, 'f1-score': 0.7668686868686868, 'support': 1125}, '1': {'precision': 0.8044444444444444, 'recall': 0.6435555555555555, 'f1-score': 0.7150617283950617, 'support': 1125}, 'accuracy': 0.7435555555555555, 'macro avg': {'precision': 0.7537037037037038, 'recall': 0.7435555555555555, 'f1-score': 0.7409652076318742, 'support': 2250}, 'weighted avg': {'precision': 0.7537037037037038, 'recall': 0.7435555555555555, 'f1-score': 0.7409652076318741, 'support': 2250}}\n",
      "<class 'dict'>\n",
      "{'0': {'precision': 0.7105064247921391, 'recall': 0.8355555555555556, 'f1-score': 0.7679738562091503, 'support': 1125}, '1': {'precision': 0.8004314994606256, 'recall': 0.6595555555555556, 'f1-score': 0.7231968810916178, 'support': 1125}, 'accuracy': 0.7475555555555555, 'macro avg': {'precision': 0.7554689621263824, 'recall': 0.7475555555555555, 'f1-score': 0.745585368650384, 'support': 2250}, 'weighted avg': {'precision': 0.7554689621263824, 'recall': 0.7475555555555555, 'f1-score': 0.745585368650384, 'support': 2250}}\n",
      "<class 'dict'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chengyiyang/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 0.5, 'recall': 1.0, 'f1-score': 0.6666666666666666, 'support': 1125}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1125}, 'accuracy': 0.5, 'macro avg': {'precision': 0.25, 'recall': 0.5, 'f1-score': 0.3333333333333333, 'support': 2250}, 'weighted avg': {'precision': 0.25, 'recall': 0.5, 'f1-score': 0.3333333333333333, 'support': 2250}}\n",
      "<class 'dict'>\n",
      "{'0': {'precision': 0.7105064247921391, 'recall': 0.8355555555555556, 'f1-score': 0.7679738562091503, 'support': 1125}, '1': {'precision': 0.8004314994606256, 'recall': 0.6595555555555556, 'f1-score': 0.7231968810916178, 'support': 1125}, 'accuracy': 0.7475555555555555, 'macro avg': {'precision': 0.7554689621263824, 'recall': 0.7475555555555555, 'f1-score': 0.745585368650384, 'support': 2250}, 'weighted avg': {'precision': 0.7554689621263824, 'recall': 0.7475555555555555, 'f1-score': 0.745585368650384, 'support': 2250}}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "#Support Vector Classifier without Gridsearch\n",
    "\n",
    "\n",
    "#tup_bots_svc = bots(tuple_of_generators[0], tuple_of_generators[1], SVC())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 0.7456896551724138, 'recall': 0.7688888888888888, 'f1-score': 0.7571115973741794, 'support': 1125}, '1': {'precision': 0.7614678899082569, 'recall': 0.7377777777777778, 'f1-score': 0.7494356659142213, 'support': 1125}, 'accuracy': 0.7533333333333333, 'macro avg': {'precision': 0.7535787725403353, 'recall': 0.7533333333333333, 'f1-score': 0.7532736316442004, 'support': 2250}, 'weighted avg': {'precision': 0.7535787725403355, 'recall': 0.7533333333333333, 'f1-score': 0.7532736316442004, 'support': 2250}}\n",
      "<class 'dict'>\n",
      "{'0': {'precision': 0.73502722323049, 'recall': 0.72, 'f1-score': 0.7274360125729681, 'support': 1125}, '1': {'precision': 0.725609756097561, 'recall': 0.7404444444444445, 'f1-score': 0.7329520457545093, 'support': 1125}, 'accuracy': 0.7302222222222222, 'macro avg': {'precision': 0.7303184896640255, 'recall': 0.7302222222222222, 'f1-score': 0.7301940291637388, 'support': 2250}, 'weighted avg': {'precision': 0.7303184896640255, 'recall': 0.7302222222222222, 'f1-score': 0.7301940291637387, 'support': 2250}}\n",
      "<class 'dict'>\n",
      "{'0': {'precision': 0.7317275747508306, 'recall': 0.7831111111111111, 'f1-score': 0.7565478746243024, 'support': 1125}, '1': {'precision': 0.7667304015296367, 'recall': 0.7128888888888889, 'f1-score': 0.7388300322432058, 'support': 1125}, 'accuracy': 0.748, 'macro avg': {'precision': 0.7492289881402336, 'recall': 0.748, 'f1-score': 0.7476889534337541, 'support': 2250}, 'weighted avg': {'precision': 0.7492289881402336, 'recall': 0.748, 'f1-score': 0.7476889534337542, 'support': 2250}}\n",
      "<class 'dict'>\n",
      "{'0': {'precision': 0.7425828970331588, 'recall': 0.7564444444444445, 'f1-score': 0.7494495816820784, 'support': 1125}, '1': {'precision': 0.7518115942028986, 'recall': 0.7377777777777778, 'f1-score': 0.7447285778375953, 'support': 1125}, 'accuracy': 0.7471111111111111, 'macro avg': {'precision': 0.7471972456180287, 'recall': 0.7471111111111111, 'f1-score': 0.7470890797598368, 'support': 2250}, 'weighted avg': {'precision': 0.7471972456180287, 'recall': 0.7471111111111111, 'f1-score': 0.7470890797598368, 'support': 2250}}\n",
      "<class 'dict'>\n",
      "{'0': {'precision': 0.7461273666092944, 'recall': 0.7706666666666667, 'f1-score': 0.7581985133362484, 'support': 1125}, '1': {'precision': 0.7628676470588235, 'recall': 0.7377777777777778, 'f1-score': 0.7501129688206055, 'support': 1125}, 'accuracy': 0.7542222222222222, 'macro avg': {'precision': 0.7544975068340589, 'recall': 0.7542222222222222, 'f1-score': 0.754155741078427, 'support': 2250}, 'weighted avg': {'precision': 0.7544975068340589, 'recall': 0.7542222222222222, 'f1-score': 0.754155741078427, 'support': 2250}}\n",
      "<class 'dict'>\n",
      "{'0': {'precision': 0.7321100917431193, 'recall': 0.7093333333333334, 'f1-score': 0.7205417607223475, 'support': 1125}, '1': {'precision': 0.718103448275862, 'recall': 0.7404444444444445, 'f1-score': 0.7291028446389497, 'support': 1125}, 'accuracy': 0.7248888888888889, 'macro avg': {'precision': 0.7251067700094906, 'recall': 0.7248888888888889, 'f1-score': 0.7248223026806486, 'support': 2250}, 'weighted avg': {'precision': 0.7251067700094906, 'recall': 0.7248888888888889, 'f1-score': 0.7248223026806486, 'support': 2250}}\n",
      "<class 'dict'>\n",
      "{'0': {'precision': 0.7425828970331588, 'recall': 0.7564444444444445, 'f1-score': 0.7494495816820784, 'support': 1125}, '1': {'precision': 0.7518115942028986, 'recall': 0.7377777777777778, 'f1-score': 0.7447285778375953, 'support': 1125}, 'accuracy': 0.7471111111111111, 'macro avg': {'precision': 0.7471972456180287, 'recall': 0.7471111111111111, 'f1-score': 0.7470890797598368, 'support': 2250}, 'weighted avg': {'precision': 0.7471972456180287, 'recall': 0.7471111111111111, 'f1-score': 0.7470890797598368, 'support': 2250}}\n",
      "<class 'dict'>\n",
      "{'0': {'precision': 0.5014985014985015, 'recall': 0.8924444444444445, 'f1-score': 0.6421490246242405, 'support': 1125}, '1': {'precision': 0.5120967741935484, 'recall': 0.11288888888888889, 'f1-score': 0.18499635833940276, 'support': 1125}, 'accuracy': 0.5026666666666667, 'macro avg': {'precision': 0.5067976378460249, 'recall': 0.5026666666666667, 'f1-score': 0.41357269148182163, 'support': 2250}, 'weighted avg': {'precision': 0.5067976378460249, 'recall': 0.5026666666666667, 'f1-score': 0.4135726914818216, 'support': 2250}}\n",
      "<class 'dict'>\n",
      "{'0': {'precision': 0.7425828970331588, 'recall': 0.7564444444444445, 'f1-score': 0.7494495816820784, 'support': 1125}, '1': {'precision': 0.7518115942028986, 'recall': 0.7377777777777778, 'f1-score': 0.7447285778375953, 'support': 1125}, 'accuracy': 0.7471111111111111, 'macro avg': {'precision': 0.7471972456180287, 'recall': 0.7471111111111111, 'f1-score': 0.7470890797598368, 'support': 2250}, 'weighted avg': {'precision': 0.7471972456180287, 'recall': 0.7471111111111111, 'f1-score': 0.7470890797598368, 'support': 2250}}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "#RandomForestClassifier without Gridsearch\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "tup_bots_forest = bots(tuple_of_generators[0], tuple_of_generators[1], RandomForestClassifier(n_jobs=-1, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.ClassifierBot object at 0x0000020267B36A88>\n"
     ]
    }
   ],
   "source": [
    "best_rf = best_clf(tup_bots_forest, 1) #best performing classifier\n",
    "print(best_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<list_iterator object at 0x0000020267C8BB88>\n"
     ]
    }
   ],
   "source": [
    "print(tup_bots_forest[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hi(r):\n",
    "    lst = [1, 2, 3, 4, 5, 6, 7]\n",
    "    print(lst[0])\n",
    "    new_lst = []\n",
    "    for i in range(0, r):\n",
    "        clf = lst.pop()\n",
    "        new_lst.append(clf)\n",
    "    return new_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[7, 6, 5]\n",
      "1\n",
      "[]\n",
      "1\n",
      "[7]\n"
     ]
    }
   ],
   "source": [
    "print(hi(3))\n",
    "print(hi(0))\n",
    "print(hi(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#classification report\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,recall_score,f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def report(prediction, real_labels, data):\n",
    "    print(\"classification report as follows: \")\n",
    "    print(f'   Accuracy: {accuracy_score(prediction, real_labels)}')\n",
    "    print(f'   Precision: {precision_score(prediction, real_labels,average=\"macro\")}')\n",
    "    print(f'   recall: {recall_score(prediction, real_labels,average=\"macro\")}')\n",
    "    print(f'   F1 measure: {f1_score(prediction, real_labels,average=\"macro\")}')\n",
    "    print('Show 5 example of correctly classified datapoint: ')\n",
    "    if data[prediction==real_labels].shape[0] > 5:\n",
    "        display(data[prediction==real_labels].iloc[:5,:])\n",
    "    else:\n",
    "        display(data[prediction==real_labels])\n",
    "    print('Show 5 example of wrongly classified datapoint: ')\n",
    "    if data[prediction!=real_labels].shape[0] > 5:\n",
    "        display(data[prediction!=real_labels].iloc[:5,:])\n",
    "    else:\n",
    "        display(data[prediction!=real_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report as follows: \n",
      "   Accuracy: 0.8168943476626144\n",
      "   Precision: 0.7084592624109877\n",
      "   recall: 0.7811589138333501\n",
      "   F1 measure: 0.7309291098045785\n",
      "Show 5 example of correctly classified datapoint: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23121</th>\n",
       "      <td>rt strategic vote kat food truly awful #mkr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35928</th>\n",
       "      <td>pancakes proof deity love us #mkr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45316</th>\n",
       "      <td>sick see fuck asshole bitch make chain latters...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42230</th>\n",
       "      <td>i'm try get insight trans issue definitely gro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40128</th>\n",
       "      <td>make</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tokens  Label\n",
       "23121        rt strategic vote kat food truly awful #mkr      0\n",
       "35928                  pancakes proof deity love us #mkr      0\n",
       "45316  sick see fuck asshole bitch make chain latters...      1\n",
       "42230  i'm try get insight trans issue definitely gro...      0\n",
       "40128                                               make      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show 5 example of wrongly classified datapoint: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48658</th>\n",
       "      <td>lol ralph guy still moi era</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30437</th>\n",
       "      <td>kill muslims oppose kill ezidis christians non...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28396</th>\n",
       "      <td>single men cannot adopt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44924</th>\n",
       "      <td>muslim brotherhood usa hundred years liken say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7346</th>\n",
       "      <td>rt #mosul christian pastor #paul_jacob sentenc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tokens  Label\n",
       "48658                        lol ralph guy still moi era      1\n",
       "30437  kill muslims oppose kill ezidis christians non...      0\n",
       "28396                            single men cannot adopt      1\n",
       "44924  muslim brotherhood usa hundred years liken say...      0\n",
       "7346   rt #mosul christian pastor #paul_jacob sentenc...      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_x = vectorizer.transform(validation_set['Tokens'])\n",
    "val_x = transformer.transform(val_x)\n",
    "\n",
    "predict = ntwk.predict(val_x)\n",
    "report(predict, validation_set['label'], validation_set[['Tokens','Label']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
